"""
Annealing algorithm for hyperopt

Annealing is a simple but effective variant on random search that
takes some advantage of a smooth response surface.

The simple (but not overly simple) code of simulated annealing makes this file
a good starting point for implementing new search algorithms.

"""

__authors__ = "James Bergstra"
__license__ = "3-clause BSD License"
__contact__ = "github.com/jaberg/hyperopt"

import logging
from collections import deque

import numpy as np
import pyll
from pyll import scope
from .base import miscs_to_idxs_vals
from .base import miscs_update_idxs_vals

logger = logging.getLogger(__name__)


def make_suggest_many_from_suggest_one(suggest_one):
    """
    Decorator to turn a suggest() function that makes a single prediction
    into one that makes N predictions.

    N predictions are generated by iteratively adding the previous suggested points
    into a temporary Trials object, and feeding that temporary trials object to remaining
    suggest() calls.
    """
    def suggest_many(new_ids, domain, trials, *args, **kwargs):
        if len(new_ids) > 1:
            # -- greedy loop rolling forward
            trials_copy = Trials()
            trials_copy._dynamic_trials = trials.trials
            trials_copy.refresh()
            rval = []
            for new_id in new_ids:
                new_trials1 = suggest_one([new_id], domain, trials_copy,
                        *args, **kwargs)
                trials_copy.insert_trial_docs(new_trials1)
                trials_copy.refresh()
                rval.extend(new_trials1)
            return rval
        else:
            return suggest_one(new_ids, domain, trials, *args, **kwargs)
    suggest_many.__name__ = suggest_one.__name__
    return suggest_many


class expr_evaluator(object):
    def __init__(self, expr,
        deepcopy_inputs=False,
        max_program_len=None,
        memo_gc=True,
        ):
        """
        Parameters:
        -----------

        expr - pyll Apply instance to be evaluated

        memo - optional dictionary of values to use for particular nodes

        deepcopy_inputs - deepcopy inputs to every node prior to calling that
            node's function on those inputs. If this leads to a different return
            value, then some function (XXX add more complete DebugMode
            functionality) in your graph is modifying its inputs and causing
            mis-calculation. XXX: This is not a fully-functional DebugMode because
            if the offender happens on account of the toposort order to be the last
            user of said input, then it will not be detected as a potential
            problem.

        """
        self.expr = pyll.as_apply(expr)
        if deepcopy_inputs not in (0, 1, False, True):
            # -- I've been calling rec_eval(expr, memo) by accident a few times
            #    this error would have been appreciated.
            raise ValueError('deepcopy_inputs should be bool', deepcopy_inputs)
        self.deepcopy_inputs = deepcopy_inputs
        if max_program_len == None:
            self.max_program_len = pyll.base.DEFAULT_MAX_PROGRAM_LEN
        else:
            self.max_program_len = max_program_len
        self.memo_gc = memo_gc

    def eval_nodes(self, memo=None):
        if memo is None:
            memo = {}
        else:
            memo = dict(memo)

        # TODO: optimize dfs to not recurse past the items in memo
        #       this is especially important for evaluating Lambdas
        #       which cause rec_eval to recurse
        #
        # N.B. that Lambdas may expand the graph during the evaluation
        #      so that this iteration may be an incomplete
        if self.memo_gc:
            clients = self.clients = {}
            for aa in pyll.dfs(self.expr):
                clients.setdefault(aa, set())
                for ii in aa.inputs():
                    clients.setdefault(ii, set()).add(aa)

        todo = deque([self.expr])
        while todo:
            if len(todo) > self.max_program_len:
                raise RuntimeError('Probably infinite loop in document')
            node = todo.pop()

            if node in memo:
                # -- we've already computed this, move on.
                continue

            # -- different kinds of nodes are treated differently:
            if node.name == 'switch':
                waiting_on = self.on_switch(memo, node)
                if waiting_on is None:
                    continue
            elif isinstance(node, pyll.Literal):
                # -- constants go straight into the memo
                self.set_memo(memo, node, node.obj)
                continue
            else:
                # -- normal instruction-type nodes have inputs
                waiting_on = [v for v in node.inputs() if v not in memo]

            if waiting_on:
                # -- Necessary inputs have yet to be evaluated.
                #    push the node back in the queue, along with the
                #    inputs it still needs
                todo.append(node)
                todo.extend(waiting_on)
            else:
                rval = self.on_node(memo, node)
                if isinstance(rval, pyll.Apply):
                    # -- if an instruction returns a Pyll apply node
                    # it means evaluate that too. Lambdas do this.
                    #
                    # XXX: consider if it is desirable, efficient, buggy
                    #      etc. to keep using the same memo dictionary
                    foo = rec_eval(rval, deepcopy_inputs, memo,
                            memo_gc=memo_gc)
                    self.set_memo(memo, node, foo)
                else:
                    self.set_memo(memo, node, rval)
        return memo

    def set_memo(self, memo, k, v):
        if self.memo_gc:
            assert v is not pyll.base.GarbageCollected
            memo[k] = v
            for ii in k.inputs():
                # -- if all clients of ii are already in the memo
                #    then we can free memo[ii] by replacing it
                #    with a dummy symbol
                if all(iic in memo for iic in self.clients[ii]):
                    #print 'collecting', ii
                    memo[ii] = pyll.base.GarbageCollected
        else:
            memo[k] = v

    def on_switch(self, memo, node):
        # -- switch is the conditional evaluation node
        switch_i_var = node.pos_args[0]
        if switch_i_var in memo:
            switch_i = memo[switch_i_var]
            try:
                int(switch_i)
            except:
                raise TypeError('switch argument was', switch_i)
            if switch_i != int(switch_i) or switch_i < 0:
                raise ValueError('switch pos must be positive int',
                        switch_i)
            rval_var = node.pos_args[switch_i + 1]
            if rval_var in memo:
                self.set_memo(memo, node, memo[rval_var])
                return 
            else:
                return [rval_var]
        else:
            return [switch_i_var]

    def on_node(self, memo, node):
        # -- not waiting on anything;
        #    this instruction can be evaluated.
        args = _args = [memo[v] for v in node.pos_args]
        kwargs = _kwargs = dict([(k, memo[v])
            for (k, v) in node.named_args])

        if self.memo_gc:
            for aa in args + kwargs.values():
                assert aa is not pyll.base.GarbageCollected

        if self.deepcopy_inputs:
            args = copy.deepcopy(_args)
            kwargs = copy.deepcopy(_kwargs)

        return scope._impls[node.name](*args, **kwargs)


class suggest_algo(expr_evaluator):
    def __init__(self, domain, trials, seed):
        expr_evaluator.__init__(self, domain.s_idxs_vals)
        self.domain = domain
        self.trials = trials
        self.label_by_node = dict([
            (n, l) for l, n in self.domain.vh.vals_by_label().items()])
        self._seed = seed
        self.rng = np.random.RandomState(seed)

    def __call__(self, new_id):
        self.rng.seed(self._seed + new_id)
        memo = self.eval_nodes(
            memo={self.domain.s_new_ids: [new_id]})
        idxs, vals = memo[self.expr]
        new_result = self.domain.new_result()
        new_misc = dict(
            tid=new_id,
            cmd=self.domain.cmd,
            workdir=self.domain.workdir)
        miscs_update_idxs_vals([new_misc], idxs, vals)
        rval = self.trials.new_trial_docs(
            [new_id], [None], [new_result], [new_misc])
        return rval

    def on_node(self, memo, node):
        if node in self.label_by_node:
            label = self.label_by_node[node]
            return self.on_node_hyperparameter(memo, node, label)
        return expr_evaluator.on_node(self, memo, node)


class annealing_algo(suggest_algo):
    """
    This simple annealing algorithm begins by sampling from the prior,
    but tends over time to sample from points closer and closer to the best
    ones observed.

    This algorithm is a simple variation on random search that leverages
    smoothness in the response surface.  The annealing rate is not adaptive.

    In addition to the value of this algorithm as a baseline optimization
    strategy, it is a simple starting point for implementing new algorithms.

    """

    def __init__(self, domain, trials,
            avg_best_idx=2.0,
            per_dimension_half_life=10.0,
            seed=123):
        suggest_algo.__init__(self, domain, trials, seed=seed)
        self.avg_best_idx = avg_best_idx
        self.per_dimension_half_life = per_dimension_half_life
        doc_by_tid = {}
        for doc in trials.trials:
            # get either this docs own tid or the one that it's from
            tid = doc['tid']
            loss = domain.loss(doc['result'], doc['spec'])
            if loss is None:
                # -- associate infinite loss to new/running/failed jobs
                loss = float('inf')
            else:
                loss = float(loss)
            doc_by_tid[tid] = (doc, loss)
        self.tid_docs_losses = sorted(doc_by_tid.items())
        self.tids = np.asarray([t for (t, (d, l)) in self.tid_docs_losses])
        self.losses = np.asarray([l for (t, (d, l)) in self.tid_docs_losses])
        self.node_tids, self.node_vals = miscs_to_idxs_vals(
            [d['misc'] for (tid, (d, l)) in self.tid_docs_losses],
            keys=domain.params.keys())

    def on_node_hyperparameter(self, memo, node, label):
        """
        Return a new value for one hyperparameter.

        Parameters:
        -----------

        memo - a partially-filled dictionary of node -> list-of-values
               for the nodes in a vectorized representation of the
               original search space.

        node - an Apply instance in the vectorized search space,
               which corresponds to a hyperparameter

        label - a string, the name of the hyperparameter


        Returns: a list with one value in it: the suggested value for this
        hyperparameter


        Notes
        -----

        This function works by delegating to self.hp_HPTYPE functions to
        handle each of the kinds of hyperparameters in hyperopt.pyll_utils.

        Other search algorithms can implement this function without
        delegating based on the hyperparameter type, but it's a pattern
        I've used a few times so I show it here.

        """
        vals = self.node_vals[label]
        losses = self.losses[self.node_tids[label]]
        losses_vals = sorted(zip(losses, vals))
        #print losses_vals
        if len(vals) == 0:
            return expr_evaluator.on_node(self, memo, node)
        else:
            best_idx = int(self.rng.geometric(1.0 / self.avg_best_idx))
            best_idx = min(best_idx, len(losses_vals) - 1)
            try:
                handler = getattr(self, 'hp_%s' % node.name)
            except AttributeError:
                raise NotImplementedError('Annealing', node.name)
            return handler(
                memo, node, label, losses_vals, best_idx)

    def hp_uniform(self, memo, node, label, losses_vals, best_idx):
        """
        Return a new value for a uniform hyperparameter.

        Parameters:
        -----------

        memo - (see on_node_hyperparameter)

        node - (see on_node_hyperparameter)

        label - (see on_node_hyperparameter)

        losses_vals - a sorted list of (loss, value) pairs that have been
                      observed for the current node.  The full history of all
                      trials can be accessed via self.trials.

        best_idx - the position within losses_vals whose value should be used as
                   as the center of the sampling distribution for annealing.

        Returns: a list with one value in it: the suggested value for this
        hyperparameter
        """
        best_val = losses_vals[best_idx][1]
        high = node.arg['high']._obj
        low = node.arg['low']._obj
        T = len(losses_vals)
        width = (high - low) * (
            0.5 ** (T / self.per_dimension_half_life))
        new_high = min(high, best_val + width / 2)
        new_low = max(low, best_val - width / 2)
        #print 'new bounds', new_low, new_high, width
        return [self.rng.uniform(low=new_low, high=new_high)]


@make_suggest_many_from_suggest_one
def suggest(new_ids, domain, trials, *args, **kwargs):
    new_id, = new_ids
    return annealing_algo(domain, trials, *args, **kwargs)(new_id)


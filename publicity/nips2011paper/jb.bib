%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Articles
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%aaaa % search target
@article{abumostafa:1989,
    author={Y. S. Abu-Mostafa},
    title={The {V}apnik-{C}hervonenkis Dimension: Information versus Complexity in Learning},
    year=1989,
    journal=nc,
    volume=1,
    number=3,
    pages={312-317},
    doi={10.1162/neco.1989.1.3.312},
    annote={}
}
@article{AdelsonBergen1985,
    author={E. H. Adelson and J. R. Bergen},
    title={Spatiotemporal Energy Models for the Perception of Motion},
    journal={Journal of the Optical Society of America},
    volume=2,
    number=2,

    year=1985,
    pages={284--299},
    annote={Introduces energy model of complex cell response.}
}
@article{allwein+shapire+singer:2000,
    author={E. L. Allwein and R. E. Shapire and Y. Singer},
    title={Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers},
    journal=jmlr,
    volume=1,
    pages={113-141},
    year=2000,
    annote={}
}
@article{alonso+chen:2009,
    title={Receptive Field},
    author={J. Alonso and Y. Chen},
    year={2009},
    journal={Scholarpedia},
    volume=4,
    number=1,
    pages={5393},
    doi={10.4249/scholarpedia.5393},
}
@article{alted:2010,
    author={F. Alted},
    title={Why Modern {CPU}s Are Starving And What Can Be Done About It},
    journal={Computing in Science and Engineering},
    volume=12,
    number=2,
    pages={68-71},
    year=2010,
    annote={}
}
@book{Anderson1990,
    title={The Adaptive Character of Thought},
    author={J.R. Anderson},
    publisher={Lawrence Erlbaum Associates},
    year={1990},
}
@article{antonov+saleev:1979,
    title={An Economic Method of Computing {$LP_\tau$}-sequences},
    author={I. A. Antonov and V. M. Saleev},
    journal={{USSR} Computational Mathematics and Mathematical Physics},
    volume=19,
    number=1,
    pages={252-256},
    year={1979},
    annote={Introduces the Sobol low-discrepancy sequence.}
}
%bbb % search target
@article{barlow:1961,
    author={H. B. Barlow},
    title={Possible Principles Underlying the Transformation of Sensory
        Messages},
    journal={Sensory Communication},
    pages={217--234},
    year=1961,
    annote={}
}
@article{barr+golden+kelly+resende+stewart:1995,
    author = {Barr, R. and Golden, B. and Kelly, J. and Resende, M. and Stewart, W.},
    title = {Designing and Reporting on Computational Experiments with Heuristic Methods},
    journal = {Journal of Heuristics},
    publisher = {Springer Netherlands},
    pages = {9-32},
    volume = {1},
    number = {1},
    doi = {10.1007/BF02430363},
    year = {1995},
    annote={A curious article - a guide to conducting and reporting empirical research on heuristic
        optimization methods. It reads like a refresher course in the scientific
            method as applied to operations research.  There is a note at the end of

            the article that it is a response to their frustrations at being unable to
            reproduce several results from the literature because insufficient detail was
            provided by the original authors.}
}
@inproceedings{barrington+yazdani+turnbull+lanckriet:2008,
    author={L. Barrington and M. Yazdani and D. Turnbull and G. Lanckriet},
    title={Combining Feature Kernels For Semantic Music Retrieval},
    booktitle={{ISMIR}},
    year={2008},
    annote={
        Combines SVM kernels for several music features, and predicts tags on CAL500 music dataset.
        Performance is measured using search criterion (ROC and mean average precision) instead of classification accuracy.
    }
}
@BOOK{battiti+brunato+mascia:2008,
    AUTHOR = {R. Battiti and M. Brunato and F. Mascia},
    TITLE = {Reactive Search and Intelligent Optimization},
    PUBLISHER = {Springer Verlag},
    YEAR = 2008,
    series = {Operations research/Computer Science Interfaces},
    volume = 45,
    annote={Outlines various heuristics for online adaptation of the parameters of
    search methods.  This book is a companion to a second book aimed at Business

    Intelligence, and a spin-off company.  There is companion software available
    but it is not free or open.}
}
@book{bear+connors+paradiso:2007,
    author={M. F. Bear and B. Connors and M. Paradiso},
    year=2007,
    title={Neuroscience: Exploring the Brain},
    address={Hagerstown, MD},
    publisher={Lippincott Williams \& Wilkins},
    isbn={0-7817-6003-8},
}
@article{becker+hinton:1993,
    author = {Becker, S. and Hinton, G. E.},
    title=  {Learning Mixture Models of Spatial Coherence},
    journal={Neural Computation},
    volume={5},
    number=2,
    pages={267--277},
    year={1993},
    annote={Uses principle of spatial coherence (like slowness, but in space) to
    guide learning.}
}
@InProceedings{behnel+bradshaw+seljebotn:2009,
    author =       {S. Behnel and R. W. Bradshaw and D. S. Seljebotn},
    title =        {Cython tutorial},
    booktitle =   {Proceedings of the 8th Python in Science Conference},
    pages =     {4 - 14},

    address = {Pasadena, CA USA},
    year =      {2009},
    editor =    {G. Varoquaux and S. {van der Walt} and J. Millman},
    howpublished = "{\url{http://conference.scipy.org/proceedings/SciPy2009/paper_1}}",
    annote = {This is the standard reference for Cython.}
}
@article{bell+sejnowski:1995,
    author={Bell, A. J. and Sejnowski, T. J.},
    title={An Information-Maximization Approach to
        Blind Separation and Blind Deconvolution},
    journal={Neural Computation},
    volume=7,
    pages={1129--1159},
    year={1995},
    annote={}
}
@article{bell+sejnowski:1997,
    author={A. J. Bell and T. J. Sejnowski},
    year=1997,
    title={The 'Independent Components' of Natural
        Scenes are Edge Filters},
    journal={Vision Research},
    volume=37,
    pages={3327-3338},

    annote={}
}
@Book{bellman:1961,
    author =       "R. Bellman",
    title =        "Adaptive Control Processes: {A} Guided Tour",
    publisher =    "Princeton University Press",
    address =      "New Jersey",
    year =         "1961",
    annote = {This is often cited as coining the term ``Curse of Dimensionality.''},
}
@Article{Bengio-trnn93,
  author =       "Y. Bengio and P. Simard and P. Frasconi",
  title =        "Learning Long-Term Dependencies with Gradient Descent is Difficult",
  journal =      ieeetrnn,
  volume =       "5",
  number =       "2",
  pages =        "157--166",
  year =         "1994",
  annote = {
      Analysis of the Jacobian of iterated dynamical systems reveals a
          fundamental tradeoff related to gradient-based learning of long-term
          dependencies in recurrent neural networks.
          If an iterated system latches information in a manner robust
          to noise, then it must have an exponentially small Jacobian and be

          un-trainable by gradient-based methods.
          On the other hand, if the Jacobian is large enough for efficient learning
          the iterated system will be unable to store information robustly for
          may iterations.
          Consequently, the phenomenon of remembering an event in a recurrent network 
          actually attenuates the capacity to adjust the detection of that
          event.
  }
}
@article{bengio:2009,
    author={Y. Bengio},
    title={Learning Deep Architectures for {AI}},
    year=2009,
    journal={Foundations and Trends in Machine Learning},
    volume=2,
    number=1,
    pages={1-127},
    doi={10.1561/2200000006}
}
@inproceedings{bengio+delalleau+leroux:2006,
    author={Y. Bengio and O. Delalleau and N. {Le Roux}},
    title={The Curse of Highly Variable Functions for Local Kernel Machines},
    pages = {107--114},
    crossref={NIPS18},

}
@inproceedings{bengio+lamblin+popovici+larochelle:2007,
    author = {Bengio, Y. and Lamblin, P. and Popovici, D. and Larochelle, H.},
    title = {Greedy Layer-Wise Training of Deep Networks},
    pages = {153--160},
    crossref={NIPS19},
    annote={Describes Gaussian-Bernoulli RBM.}
}
@INPROCEEDINGS{bengio+glorot:2010,
     author = {Bengio, Y. and Glorot, X.},
      title = {Understanding the difficulty of training deep feedforward neural networks},
      crossref = {AISTATS:2010},
      pages = {249-256},
   abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficia
l. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@INCOLLECTION{bengio+lecun:2007,
    author = {Bengio, Y. and {LeCun}, Y.},
    editor = {Bottou, L. and Chapelle, O. and DeCoste, D. and Weston, J.},
    title = {Scaling Learning Algorithms towards {AI}},
    booktitle = {Large Scale Kernel Machines},
    year = {2007},
    publisher = {MIT Press},
    abstract = {One long-term goal of machine learning research is to produce methods that
    are applicable to highly complex tasks, such as perception (vision, audition), reasoning,
    intelligent control, and other artificially intelligent behaviors. We argue
    that in order to progress toward this goal, the Machine Learning community must
    endeavor to discover algorithms that can learn highly complex functions, with minimal
    need for prior knowledge, and with minimal human intervention. We present
    mathematical and empirical evidence suggesting that many popular approaches
    to non-parametric learning, particularly kernel methods, are fundamentally limited
    in their ability to learn complex high-dimensional functions. Our analysis
    focuses on two problems. First, kernel machines are shallow architectures, in
    which one large layer of simple template matchers is followed by a single layer

    of trainable coefficients. We argue that shallow architectures can be very inefficient
    in terms of required number of computational elements and examples. Second,
    we analyze a limitation of kernel machines with a local kernel, linked to the
    curse of dimensionality, that applies to supervised, unsupervised (manifold learning)
    and semi-supervised kernel machines. Using empirical results on invariant
    image recognition tasks, kernel methods are compared with deep architectures, in
    which lower-level features or concepts are progressively combined into more abstract
    and higher-level representations. We argue that deep architectures have the
    potential to generalize in non-local ways, i.e., beyond immediate neighbors, and
    that this is crucial in order to make progress on the kind of complex tasks required
    for artificial intelligence.},
}
@inproceedings{bengio+louradour+collobert+weston:2009,
    author={Y. Bengio and J. Louradour and R. Collobert and J. Weston},
    title={Curriculum Learning},
    pages={41--48},
    crossref={ICML:2009},
}
@inproceedings{berg+malik:2001,
    author={Berg, A. C. and Malik, J. },
    year={2001},
    title={Geometric Blur and Template Matching},
    booktitle=cvpr01,
}

@INPROCEEDINGS{bergstra+bengio:2009,
     author = {Bergstra, J. and Bengio, Y.},
      month = dec,
      title = {Slow, Decorrelated Features for Pretraining Complex Cell-like Networks},
      pages = {99--107},
      crossref={NIPS22},
}
@misc{bergstra+bengio:2011,
     author = {Bergstra, J. and Bengio, Y.},
      title = {Random Search for Hyper-parameter Optimization},
       year = {2011},
       howpublished=jmlr,
       note = {Submitted (11-077)}
}
@misc{bergstra+bengio:2012accepted,
     author = {J. Bergstra and Y. Bengio},
      title = {Random Search for Hyper-parameter Optimization},
       year = {2012},
       howpublished=jmlr,
       note = {Accepted.}
}
@misc{bergstra+bengio:2011snowbird,
     author = {Bergstra, J. and Bengio, Y.},
      title = {Random Search for Hyper-parameter Optimization},
       year = {2011},
       howpublished="The Learning Workshop (Snowbird)",
}
@ARTICLE{bergstra+bengio+louradour:2011,
    author = {Bergstra, J. and Bengio, Y. and Louradour, J.},
     month = mar,
     title = {Suitability of {V1} Energy Models for Object Classification},
   journal = nc,
    volume = {23},
    number = {3},
      year = {2011},
     pages = {774--790}
}
@unpublished{bergstra+courville+bengio:2010,
    author={J. Bergstra and A. Courville and Y. Bengio},
    title={On the Statistical Inefficiency of Sparse Coding},
    year=2010,
    note={Submitted to \citet{NIPS23}.}
}
@TECHREPORT{bergstra+desjardins+lamblin+bengio:2009,
       author = {Bergstra, J. and Desjardins, G. and Lamblin, P. and Bengio, Y.},
        month = apr,
        title = {Quadratic Polynomials Learn Better Image Features},
       number = {1337},
         year = {2009},
  institution = {D{\'e}partement d'Informatique et de Recherche Op{\'e}rationnelle, Universit{\'e} de Montr{\'e}al},
     abstract = {The affine-sigmoidal hidden unit (of the form $\sigma(ax+b)$)
    is a crude predictor of neuron response in visual area V1.
    More descriptive models of V1 have been advanced that are no more computationally expensive,
    yet artificial neural network research continues to focus on networks of affine-sigmoidal models.
    This paper identifies two qualitative differences between the affine-sigmoidal hidden unit
    and a particular recent model of V1 response:
    a) the presence of a low-rank quadratic term in the argument to $\sigma$,
    and b) the use of a gentler non-linearity than the $\tanh$ or logistic sigmoid.
    We evaluate these model ingredients by training single-layer
    neural networks to solve three image classification tasks.
    We experimented with fully-connected hidden units,
    as well as locally-connected units and convolutional units
    that more closely mimic the function and connectivity of the visual system.
    On all three tasks, both the quadratic interactions and the gentler non-linearity
    lead to significantly better generalization.
    The advantage of quadratic units was strongest in conjunction with sparse and convolutional hidden units.}
}
@inproceedings{bergstra+etal:2010,
     author = {Bergstra, J. and Breuleux, O. and Bastien, F. and Lamblin, P. and Pascanu, R. and Desjardins, G. and Turian, J. and Bengio, Y.},
      month = jun,
      title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
       year = {2010},
   location = {Austin, TX},
}
@misc{bergstra+etal:2011,
     author = {Bergstra, J. and Breuleux, O. and Bastien, F. and Lamblin, P. and Pascanu, R. and Desjardins, G. and Turian, J. and Bengio, Y.},
      title = {Deep Learning in {Python} with {Theano}},
       year = {2011},
       howpublished = JMLR,
       note = {In preparation}
}
@INPROCEEDINGS{bergstra+mandel+eck:2010,
     author = {Bergstra, J. and Mandel, M. and Eck, D.},
      month = aug,
      title = {Scalable Genre and Tag Prediction with Spectral Covariance},
  booktitle = ismir2010,
       year = {2010},
      pages = {507--512},
   location = {Utrecht, The Netherlands.}
}
@article{berkes+orban+lengyel+fiser:2010,
    author={P. Berkes and G. Orb{\`a}n and M. Lengyel and J. Fiser},
    title={Spontaneous Cortical Activity Reveals Hallmarks of an Optimal Internal Model of the Environment},
    journal={Science},
    year=2011,
    volume=331,
    number=6013,
    pages={83-87},
    doi={10.1126/science.1195870},
    annote={Evidence for Bayesian prior in visual cortex. Experiments on young, awake ferrets using both natural and artificial video. Not sure what specific prior is proposed, if any (TODO).} 
}
@article{berkes+turner+sahani:2009,
    author={P. Berkes and R. E. Turner and M. Sahani},
    title={A Structured Model of Video Reproduces Primary Visual Cortical Organizations},
    journal=plos_compbio,
    volume=5,
    number=9,
    pages={e1000495},
    year=2009,
    doi={10.1371/journal.pcbi.1000495}
}
@inproceedings{berkes+white+fiser:2009,
    author={P. Berkes and B. L. White and J. Fiser},
    title={No Evidence for Active Sparsification in the Visual Cortex},
    pages = {108--116},
    crossref={NIPS22},
}
@article{berkes+wiskott:2005,
    author = {Berkes, P. and Wiskott, L.},
    title = {Slow Feature Analysis Yields a Rich Repertoire of Complex Cell Properties},
    journal = {Journal of Vision},
    volume = {5},
    number = {6},
    pages = {579-602},
    year = {2005},
    abstract = {In this study we investigate temporal slowness as a learning principle for receptive fields using slow feature analysis, a new algorithm to determine functions that extract slowly varying signals from the input data. We find a good qualitative and quantitative match between the set of learned functions trained on image sequences and the population of complex cells in the primary visual cortex (V1). The functions show many properties found also experimentally in complex cells, such as direction selectivity, non-orthogonal inhibition, end-inhibition, and side-inhibition. Our results demonstrate that a single unsupervised learning principle can account for such a rich repertoire of receptive field properties.},
    annote = {This article examines features learned by SFA in terms of the
        qualitative and quantitative similarity to complex cells in V1.
            The discussion section (6.3) includes insightful comparisons with the
            slow-feature criteria of~\cite{kording:2004}, Einh{\"a}user2002 and
            Hurri2003.
            See also Appendix (A.7) for comparison to temporal coherence.
    }
}
@ARTICLE{bertinmahieux+eck+maillet+lamere:2008,
    AUTHOR = {T. Bertin-Mahieux and D. Eck and F. Maillet and P. Lamere},
    TITLE = {Autotagger: A Model For Predicting Social Tags from Acoustic Features on Large Music Databases},
    JOURNAL = {Journal of New Music Research},
    YEAR = {2008},
    VOLUME = {37},
    NUMBER = {2},
    PAGES = {115--135},
    PDF = {papers/2008_jnmr.pdf},
    SOURCE = {OwnPublication},
    NOTE = {To appear.},
    annote = {
        Proposes and evalutes FilterBoost for large-scale prediction of social tags (from last.fm).
        Includes a break-down of what's in last.fm: main ingredients are 68\% genre, 12\% locale 5\% mood.
        Section 6 includes comparison with "small and clean" dataset used to test the hierarchical Gaussian mixture model.
        Section 7 includes discussion of utility in music similarity.
    }
}
@Book{bishop:1995,
  author =       "C. Bishop",
  title =        "Neural Networks for Pattern Recognition",
  publisher =    "Oxford University Press",
  address =      "London, UK",
  year =         "1995",
}
@article{blasdel+salama:1986,
    author={Blasdel, G. G. and Salama, G.},
    year=1986,
    title={Voltage Sensitive Dyes Reveal a Modular Organization in Monkey Striate Cortex},
    journal={Nature},
    volume=321,
    pages={579-585},
    annote={Has really nice illustration of pinwheel organization in V1.},
}
@article{bonds:1989,
    author={A. B. Bonds},
    title={Role of Inhibition in the Specification of Orientation Selectivity of
        Cells in the Cat Striate Cortex},
    journal={Vision Neuroscience},
    volume={2},
    pages={41--55},
    year=1989,
    annote={},
}
@article{bordes+bottou+gallinari:2009,
    author={A. Bordes and L. Bottou and P. Gallinari},
    title={{SGD-QN}: Careful Quasi-Newton Stochastic Gradient Descent},
    journal={Journal of Machine Learning Research},
    volume=10,
    pages={1737-1754},
    year={2009},
    annote={Fast quasi-newton algo that is shown to work well for linear SVM.
        Makes use of sparse data vectors, good implementation, a diagonal
            rescaling matrix inspired by oLBFGS.
        This algorithm won a Pascal 'Wild Track' Challenge.  },
    note={See also \cite{bordes+etal:2010err}}
}
@article{bordes+etal:2010err,
    author =  {A. Bordes and L. Bottou and P. Gallinari and J. Chang and S. A. Smith},
    title =   {Erratum: {SGDQN} is Less Careful than Expected},
    journal = {Journal of Machine Learning Research},
    year =    {2010},
    volume =  {11},
    pages =   {2229−2240},
    month =   {Aug}
}
@inproceedings{bordes+bottou+gallinari+weston:2007,
    title={Solving MultiClass Support Vector Machines with {LaRank}},
    author={A. Bordes and L. Bottou and P. Gallinari and J. Weston},
    booktitle=icml07,
    editor=icml07ed,
    publisher=icml07publ,
    year=2007,
    pages = {89--96},
}
@inproceedings{bosch+zisserman+munoz:2007,
    author={Bosch, A. and Zisserman, A. and Munoz X.},
    year={2007},
    title={Representing Shape with a Spatial Pyramid Kernel},
    booktitle={International Conference on Image and Video Retrieval ({CIVR})},
    annote={Describes Pyramid Histogram of Oriented Gradients (PHOG) image
        features.}
}
@incollection{bottou:1998,
    author = {Bottou, L.},
    title = {Online Algorithms and Stochastic Approximations},
    booktitle = {Online Learning and Neural Networks},
    editor = {Saad, D.},
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    year = {1998},
}
@techreport{bottou:2011,
    author = {Bottou, L.},
    title = {From Machine Learning to Machine Reasoning},
    institution = {arXiv.1102.1808},
    year = {2011},
}
@article{bower1992,
    author={J. M. Bower and C. Koch},
    title={Experimentalists and modelers: Can we all just get along?},
    journal={Trends Neuroscience},
    year=1992,
    issue=15,
    pages={458--461},
}
@article{bratley+fox+niederreiter:1992,
    title={Implementation and Tests of Low-Discrepancy Sequences},
    author={P. Bratley and B. L. Fox and H. Niederreiter},
    journal={Transactions on Modeling and Computer Simulation, ({TOMACS})},
    editor={R. E. Nance},
    publisher={ACM},
    volume={2},
    number={3},
    pages={195--213},
    year={1992},
    annote={Introduces the Niederreiter low-discrepancy sequence.},
}
@article{BredfeldtRingach2002,
    author = {Bredfeldt, C. E. and Ringach, D. L.},
    title = {Dynamics of Spatial Frequency Tuning in Macaque {V1}},
    journal = {Journal of Neuroscience},
    volume = {22},
    number = {5},
    year = {2002},
    month = {March},
    pages = {1976-84},
}
@inproceedings{brown+lowe:2002,
    author={M. Brown and D. G. Lowe},
    title={Invariant Features from Interest Point Groups},
    booktitle={British Machine Vision Conference ({BMVC})},
    year=2002,
    address={Cardiff, Wales},
    month=sep,
    pages={656-665},
    annote={Describes filtering techniques to be used in SIFT interest point
        detection. Referenced in (Lowe 2004).}
}
@article{BuiaTiesinga2006,
    author={C. C. Buia and P. P. Tiesinga},
    title={Attentional Modulation of Firing Rate and Synchrony in a Model Cortical Network},
    journal={Journal of Computational Neuroscience},
    volume={20},
    number={3},
    year={2006},
    pages={247-64},
}
@phdthesis{burred:2009thesis,
    author={J. J. Burred},
    title={From Sparse Models to Timbre Learning: New Methods for Musical Source Separation},
    year={2008},
    school={Technical University of Berlin},
}
%ccc % search target
@phdthesis{cadieu:2009,
    author={C. Cadieu},
    title={Probabilistic Models of Phase Variables for Visual Representation and
        Neural Dynamics},
    year=2009,
    school={University of California, Berkeley},
}
@article{cadieu+kouh+pasupathy+conner+riesenhuber+poggio:2007,
    author={C. Cadieu and M. Kouh and A. Pasupathy and C. Connor and M.
        Riesenhuber and T. Poggio},
    title={A Model of V4 Shape Selectivity and Invariance},
    journal={Journal of Neurophysiology},
    volume={98},
    pages={1733--1750},
    month={June},
    year={2007},
}
@misc{cadieu+koepsell:2010,
    author={C. F. Cadieu and K. Koepsell},
    title={Modeling Image Structure with Factorized Phase-Coupled {Boltzmann}
        Machines},
    year=2010,
    howpublished={arXiv:1011.4058v1},
    annote={Presents an energy model similar to the mcRBM for training a
        phase-based model. Borrows HMC trick for sampling.}
}
@incollection{cadieu+olshausen:2009,
     title = {Learning Transformational Invariants from Natural Movies},
      author = {C. Cadieu and B. A. Olshausen},
       booktitle = NIPS21,
        editor = NIPS21ed,
         pages = {209--216},
          year = {2009},
     publisher = {MIT Press},
    annote={
        Proposes a two-layer, sparse coding generative model for image sequences
            in which the lower level explains the image at time t by the real
            part of a complex coefficient times a complex filter.  There is a
            sparsity penalty is on the amplitude of the coefficients.  There is
            also a slowness penalty on the amplitude of the coefficients through
            time.  The second layer imposes a joint prior over *derivatives* of
            the angular parts of the coefficients by multiplying a second-level
            units by a weight matrix.  There is again a sparsity prior on these
            second-level units, but not a slowness prior (it's a prior over
                    derivatives after all).
    }}
@misc{caflisch+morokoff+owen:1997,
    author = {R. E. Caflisch and W. Morokoff and A. Owen},
    title = {Valuation of Mortgage Backed Securities Using Brownian Bridges to Reduce Effective Dimension},
    year = {1997},
}
@article{cao:2003,
    author={J. Cao and J. Wang},
    title={Global asymptotic stability of a general class of recurrent neural
        networks with time-varying delays},
    journal={IEEE Transactions on Circuits and Systems I: Fundamental Theory and
        Applications},
    year={2003},
    volume=50,
    number=1,
    pages={34-44},
    doi={10.1109/TCSI.2002.807494},
}
@article{carandini:1994,
    author = {M. Carandini and D. J. Heeger},
    title = {Summation and Division by Neurons in Primate Visual Cortex},
    journal = {Science},
    volume={264},
    number={5163},
    month = {May},
    year = {1994},
    pages = {1333-1336},
}
@article{carandini:2006,
    author={Carandini, M.},
    year={2006},
    title={What Simple and Complex Cells Compute},
    journal={Journal of Physiology},
    volume={577},
    pages={463--466},
    annote={}
}
@article{cavanaugh+bair+movshon:2002,
    author={Cavanaugh, J. R. and Bair, W. and Movshon, J. A.},
    title={Nature and Interaction of Signals from the Receptive Field Center and
        Surround in Macaque {V1} Neurons},
    journal={Journal of  Neurophysiology},
    volume={88},
    pages={2530--2546},
    year={2002},
    annote={},
}
@Manual{chang+lin:2001,
    author =   {C. Chang and C. Lin},
    title =  {{LIBSVM}: A Library for Support Vector Machines},
    year =     {2001},
}
@article{chen+etal:2009,
    author={Y. Chen and S. Anand and S. Martinez-Conde and S. L. Macknik and Y.
    Bereshpolova and H. A. Swadlow and J. Alonso},
    year={2009},
    title={The Linearity and Selectivity of Neuronal Responses
        in Awake Visual Cortex},
    journal={Journal of Vision},
    volume=9,
    number=9,
    pages={1--17},
    annote={},
}
@article{chen+garcia+gupta+rahimi+cazzanti:2009,
    title={Similarity-based Classification: Concepts and Algorithms},
    author={Y. Chen and E. K. Garcia and M. R. Gupta and A. Rahimi and L. Cazzanti},
    pages={747--776},
    journal = {JMLR},
    volume = {10},
    month="March",
    year={2009},
}
@article{chen+murray:2003,
  author =       "H. Chen and A. F. Murray",
  title =        "A Continuous Restricted {Boltzmann} Machine with an Implementable Training Algorithm",
  journal =      "IEE Proceedings of Vision, Image and Signal Processing",
  volume =       "150",
  number =       "3",
  pages =        "153--158",
  year =         "2003",
  doi = {10.1049/ip-vis:20030362},
  annote = {Uses CD to train a continuous-valued RBM that is based on injective noise and diffusion. I do not think this is the same as Gaussian RBM, but the paper is hard to read.}
}
@misc{coates+lee+ng:2010,
    title={An Analysis of Single-Layer Networks in Unsupervised Feature Learning},
    author={A. Coates and H. Lee and A. Ng},
    howpublished={NIPS Deep Learning and Unsupervised Feature Learning Workshop},
    year=2010,
}
@inproceedings{coates+lee+ng:2011,
    title={An Analysis of Single-Layer Networks in Unsupervised Feature Learning},
    author={A. Coates and H. Lee and A. Y. Ng},
    crossref={AISTATS:2011},
}
@inproceedings{coates+ng:2011,
    title={The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization},
    author={A. Coates and A. Y. Ng},
    crossref={ICML:2011}
}
@article{connor+brincat+pasupathy:2007,
    author={Connor, C. E. and Brincat, S. L. and Pasupathy, A.},
    title={Transformation of Shape Information in the Ventral Pathway},
    journal={Current Opinions in Neurobiology},
    volume=17,
    pages={140--147},
    year={2007},
    annote={},
}
@inproceedings{collobert+weston:2008,
    title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
    author = {R. Collobert and J. Weston},
    pages = {},
    crossref={ICML:2008},
}
@article{cortes+vapnik:1995,
    author={C. Cortes and V. Vapnik},
    title={Support Vector Networks},
    journal={Machine Learning},
    volume=20,
    year=1995,
    pages={273--297},
    annote={Introduces kernel SVMs, hinge loss (soft margin) and kernel trick.}
}
@misc{courville+bergstra+bengio:2010,
    title={The Spike and Slab Restricted {Boltzmann} Machine},
    author={A. Courville and J. Bergstra and Y. Bengio},
    howpublished={NIPS Deep Learning and Unsupervised Feature Learning Workshop},
    year=2010,
}
@inproceedings{courville+bergstra+bengio:2011a,
    title={The Spike and Slab Restricted {Boltzmann} Machine},
    author={A. Courville and J. Bergstra and Y. Bengio},
    crossref={AISTATS:2011},
    note={Accepted, received {People's Choice} Award.},
}
@misc{courville+bergstra+bengio:2011b,
    title={Unsupervised Models of Images by Spike and Slab {RBM}s},
    author={A. Courville and J. Bergstra and Y. Bengio},
    howpublished=ICML,
    year=2011,
    note={Submitted},
}
@article{cox+dicarlo:2008,
    author={D. D. Cox and J. J. {DiCarlo}},
    title={Does Learned Shape Selectivity in Inferior Temporal Cortex
        Automatically Generalize Across Retinal Position?},
    journal={Neuroscience},
    volume=28,
    number=40,
    pages={10045--10055},
    year={2008},
    annote={Physiological measurements in carefully trained monkeys shows that it does not. Authors hypothesize instead that the ability to generalize is learned (from experience) rather than hard-wired.}
}
@article{cox+meier+oertelt+dicarlo:2005,
    author={Cox, D. D. and Meier, P. and Oertelt, N. and {DiCarlo}, J. J.},
    title={{``Breaking''} Position Invariant Object Recognition},
    journal={Nature Neuroscience},
    volume=8,
    pages={1145--1147},
    year={2005},
    annote = {Shows that 1 hour of exposure to object changes during saccades is enough to confuse people when objects are placed at those same retinal positions. This is evidence that low level vision is adapting to environment statistics, but also that object properties are not learned in a way that automatically generalizes across retinal positions.},
}
@unpublished{cox+pinto+doukhan+cordas+dicarlo:2008,
    author={D. D. Cox and N. Pinto and D. Doukan and B. Cordas and J. J.
        {DiCarlo}},
    title={A High-Throughput Screening Approach to Discovering Good Forms of
        Visual Representation},
    year={2008},
    note={Abstract and poster, {COSYNE}},
}
@article{curcio+allen:1990,
    author={C. A. Curcio and K. A. Allen},
    title={Topography of Ganglion Cells in Human Retina},
    journal={Journal of Computational Neurology},
    year=1990,
    month=oct,
    volume=300,
    number=1,
    page={5--25},
    annote={Foveal cell density: 32K to 38K cells/mm2 in a horizontally
        elliptical ring 0.4-2.0mm from the foveal center.
    Number of ganglion cells ranges from 700 thousand to 1.5 million. They
    estimate that cones may not only be wired directly (1-1) to V1, but that
    actually in humans there might be multiple ganglion axons for each foveal
    cone - possibly to have both center-on and center-off responses.}
}
@techreport{czogiel+luebke+weihs:2005,
    author={I. Czogiel and K. Luebke and C. Weihs},
    title={Response Surface Methodology for Optimizing Hyper Parameters},
    institution={Universit{\"a}t Dortmund Fachbereich Statistik},
    year={2005},
    month={September},
    annote={Shows that on a small problem involving the optimization of SVM
        hyper-parameters, their algorithm outperforms Nelder-Mead. It appears
        that their algorithm is not easy to parallelize.},
    path={reading/2005/czogiel_luebke_weihs.pdf}
}
%ddd % search target 
@inproceedings{dahl+ranzato+mohamed+hinton:2010,
    title={Phone Recognition with the Mean-Covariance Restricted {Boltzmann} Machine},
    author={G. E. Dahl and M. Ranzato and A. Mohamed and
        and G. E. Hinton},
    booktitle=nips23,
    editor=nips23ed,
    pages = {469--477},
    year={2010},
    annote={
        An mc-RBM with some RBMs stacked on top scores new state-of-the-art 20.7\% error on speaker-independent TIMIT.
        (Bests previous state-of-the-art normal DBN on TIMIT.)
        Must ask George about juicy details, because the paper makes it sound
        like everything just worked out of the box.
    }
}
@article{dasgupta:2009,
    author={S. Dasgupta and A. T. Kalai and C. Monteleoni},
    title={Analysis of Perceptron-Based Active Learning},
    journal = {JMLR},
    year=2009,
    volume=10,
    month=February,
    pages={281--299},
}
@book{DayanAbbott2001,
    author={P. Dayan and L. F. Abbott},
    title = {Theoretical Neuroscience},
    publisher = {The {MIT} Press},
    year = 2001,
}
@Article{decoste+scholkopf:2002,
  author =       "D. Decoste and B. Sch{\"o}lkopf",
  title =        "Training invariant support vector machines",
  journal =      "Machine Learning",
  volume =       "46",
  pages =        "161--190",
  year =         "2002",
  annote = {Reports Gaussian SVM score of 1.5\% on MNIST.},
  doi={10.1023/A:1012454411458},
}
@article{doi2003,
    authors = {E. Doi and T. Inui and T. Lee and T. Wachtler and T. J. Sejnowski},
    title = {Spatiochromatic Receptive Field Properties Derived from Information-Theoretic Analyses of Cone Mosaic Responses to Natural Scenes},
    journal = {Neural Computation},
    year = 2003,
    volume= 15,
    pages = {397-417},
}
@InProceedings{Doi2007,
    author = {Doi, E. and Lewicki, M. S.},
    title = {A Theory of Retinal Population Coding},
    booktitle = {Advances in Neural Information Processing Systems 19},
    publisher = {{MIT} Press},
    address={Cambridge},
    year = {2007},
}
@article{dongarra+ducroz+duff+hammarling:1990,
    author={J. J. Dongarra and J. {Du Croz} and I. S. Duff and S. Hammarling},
    title={Algorithm 679: A set of Level 3 Basic Linear Algebra Subprograms},
    journal={ACM Transactions on Mathematical Software},
    volume=16,
    pages={18--28},
    year=1990,
    annote={This is a reference for BLAS interface and routines.}
}
@ARTICLE{dowling:2007,
    AUTHOR = {Dowling, J. },
    TITLE  = {Retina},
    YEAR     = {2007},
    JOURNAL = {Scholarpedia},
    VOLUME  = {2},
    NUMBER  = {12},
    PAGES  = {3487},
    DOI = {10.4249/scholarpedia.3487},
}
@inproceedings{drew+homem-de-mello:2006,
    title={Quasi-Monte Carlo strategies for stochastic optimization},
    author={S. S. Drew and T. Homem-de-Mello},
    booktitle={Proc. of the 38th Conference on Winter Simulation},
    year=2006,
    pages={774 - 782},
    annote={Presents an algorithm for optimization that divides dimensions of search space into important and unimportant - uses QMC to cover important dimensions, and pseudo-random samples to cover rest.}
}
@inproceedings{druckmann+chklovskii:2010,
    title={Over-complete Representations on Recurrent Neural Networks can
        Support Persistent Percepts},
    author={S. Druckmann and D. B. Chklovskii},
    booktitle={{NIPS}},
    year={2010},
    annote={
        Describes a recipe and motivation for coming up with a permutation-type
            matrix $L$ that can cycle through good (but not optimal) solutions to the sparse coding
            problem.  This model offers to accounts for how/why individual
            neurons do not stay active even though a percept is held constant.
            Although it was not presented this way, this algorithm might provide
            a way to overcome the representational instability that is inherent
            in the sparse coding framework.
        From the question period after the talk: ``In order for
        a neuron to communicate it needs to spike.  But spiking resets the
        neuron.  So a neuron loses its information in sending it.''
    }
}
%eeeeeee  % search target
@unpublished{edelman+intrator+poggio:1997,
    author={S. Edelman and N. Intrator and T. Poggio},
    title={Complex Cells and Object Recognition},
    note={Unpublished},
    year={1997}
}
@inproceedings{erhan+etal:2009,
 author = {D. Erhan and P. Manzagol and Y. Bengio and S. Bengio and P. Vincent},
  booktitle =    aistats09,
  editor = aistats09ed,
  publisher = aistats09publ,
  title = "The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training",
  pages = "153--160",
  address = "Clearwater, FL",
  year = 2009,
}
@article{erhan+etal:2010,
    author={D. Erhan and Y. Bengio and A. Courville and P. Manzagol and P. Vincent and S. Bengio},
    title={Why Does Unsupervised Pre-training Help Deep Learning?},
    year=2010,
    journal={Journal of Machine Learning Research},
    volume=11,
    pages={625-660},
}
@article{escalante:2009,
    author={H. J. Escalante, M. Montes, L. E. Sucar},
    title={Particle Swarm Model Selection},
    journal = {JMLR},
    year=2009,
    volume=10,
    month=February,
    pages={405--440},
}
%fff  % search target
@article{farley+yu+jin+sur:2007,
    author={Farley, B. F. and Yu, H. and Jin, D. Z. and Sur, M.},
    title={Alteration of Visual Input Results in a Coordinated Reorganization of Multiple Visual Cortex Maps},
    journal={Journal of Neuroscience},
    volume=27,
    pages={10299--10310},
    year={2007},
}
@article{field:1994,
    author={D. J. Field},
    year={1994},
    title={What is the Goal of Sensory Coding?},
    journal=nc,
    volume=6,
    pages={559--601},
    annote={}
}
@article{finn+ferster:2007,
    author={I. M. Finn and D. Ferster},
    title={Computational Diversity in Complex Cells of Cat Primary Visual Cortex},
    journal={Journal of Neuroscience},
    month={September},
    year={2007},
    volume={27},
    number={36},
    pages={9638--9648},
    annote={A comparison of MAX-like and energy-type models for intracellular
        recordings in V1 finds that both kinds of responses exist.  Cells used
            in the study are described as spanning a range of behaviours,
               rather than conforming to either model type.
                   "Classical responses emerged when a single pair of simple
                   cells with matched statial frequencies was used. More
                   MAX-like behavior was observed when two pairs of cells
                   with different spatial-frequency selectivities provided
                   input to the model complex cell. Thus, it may be the case
                   that complex cells participate in a variety of image
                   processing computations dependent in part on the spatial-
                   frequency preferences of the inputs they receive."
    }
}
@article{fisher:1936,
    author={R. A. Fisher},
    title={The Use of Multiple Measurements in Taxonomic Problems},
    journal={Ann. Eugenics},
    volume=7,
    number=111,
    year=1936,
}
@article{franz+gegenfurtner+bulthoff+fahle:2000,
    author={V. H. Franz and K. R. Gegenfurtner and H. H. B{\"u}lthoff and M. Fahle},
    title={Grasping Visual Illusions: No Evidence for a Dissociation Between Perception and Action},
    journal={Psychological Science},
    month=jan,
    year=2000,
    volume=11,
    number=1,
    pages={20-25},
    doi={10.1111/1467-9280.00209},
}
@article{franzius+sprekeler+wiskott:2007,
    author={Franzius, M. and Sprekeler, H. and Wiskott, L.},
    year={2007},
    title={Slowness and Sparseness
        Lead to Place, Head-direction, and Spatial-view Cells},
    journal={{PLoS} Computational Biology},
    volume={3},
    number={8},
    pages={e166},
}
@TechReport{freund+haussler:1994,
  author =       "Y. Freund and D. Haussler",
  title =        "Unsupervised Learning of Distributions on Binary Vectors using Two Layer Networks",
  number =       "UCSC-CRL-94-25",
  institution =  "University of California, Santa Cruz",
  year =         "1994",
  annote = {Discusses and derives free energy for binary and and continuous RBMs, suggests training them by using optimization methods to find the points in input space for which the probability is highest.  Misses the connection to Gibbs sampling that Hinton later builds on.}
  }
}
%ggg % search target
@manual{galassi+etal:2009,
    author={M. Galassi et al},
    title={GNU Scientific Library Reference Manual},
    year={2009},
    edition={3rd},
}
@article{gallant+etal:1996,
    author={Gallant, J. L. and Connor, C. E. and Rakshit, S. and Lewis, J. W. and Van Essen, D. C.},
    title={Neural Responses to Polar, Hyperbolic, and {Cartesian} gratings in Area {V4} of the Macaque Monkey},
    journal={Journal of Neurophysiology},
    volume=76,
    pages={2718--2739},
    year={1996},
    annote={},
}
@inproceedings{garrigues+olshausen:2009,
    author={P. J. Garrigues and B. A. Olshausen},
    title={Learning Horizontal Connections in a Sparse Coding Model of Natural
        Images},
    pages={505--512},
    crossref={NIPS20},
    annote={Sparse coding with a fully-interconnected hidden layer. Weights are
    visualized and some probabilities are plotted. It looks like MAP estimation
    of $z$ in the model is hard, but I don't see right away why.}
}
@inproceedings{gerdzhev+al:2010,
    author={M. Gerdzhev and J. Tran and A. Ferworn and K. Barnum and M. Dolderman},
    title={A Scrubbing Technique for the Automatic Detection of Victims in Urban Search and Rescue Video},
    crossref={IWCMC:2010},
}
@article{ghanty+paul+pal:2009,
    author={P. Ghanty and S. Paul and N. R. Pal},
    title = {NEUROSVM: An Architecture to Reduce the Effect of the Choice of
        Kernel on the Performance of SVM},
    journal = {JMLR},
    volume = {10},
    month="March",
    pages={591--622},
    year={2009},
}
@article{giles:1987,
    author = {C. L. Giles and T. Maxwell},
    journal = {Appl. Opt.},
    number = {23},
    pages = {4972},
    publisher = {OSA},
    title = {Learning, Invariance, and Generalization in High-Order Neural Networks},
    volume = {26},
    year = {1987},
}
@article{goodale+milner:1992,
    author={M. A. Goodale and A. D. Milner},
    title={Separate Visual Pathways for Perception and Action},
    year=1992,
    journal={Trends  in Neuroscience},
    volume=15,
    number=1,
    month=jan,
    pages={20-25},
    doi={doi:10.1016/0166-2236(92)90344-8},
}
@inproceedings{goodman+brette:2010,
    title={Learning to Localise Sounds with Spiking Neural Networks},
    author={D. F. M. Goodman and R. Brette},
    booktitle={{NIPS}},
    year={2010},
    annote={
        Describe a neural network architecture in in which supervised learning
        is used to learn complements to head-related transfer functions
        (HRTFs) for single sources at various positions in the acoustic
        environment.
        They use a mesh grid (aka lattice) of filters that can cancel out the
        effect of HRTFs, and then learn to match the responses of that family
        (composed with the unknown HRTF) to a known acoustic positions.  This
        method is tested on synthetic and recorded sounds.  Experiments are done
        with both spiking and non-spiking models.
    }
}
@article{gray:1999,
    author={C. M. Gray},
    title={The Temporal Correlation Hypothesis of Visual Feature Integration: Still Alive and Well},
    journal={Neuron},
    volume=24,
    pages={31-47},
    year={1999},
}
@inproceedings{gregor+lecun:2010,
    author={K. Gregor and Y. {LeCun}},
    title={Learning Fast Approximations of Sparse Coding},
    booktitle=icml10,
    editor=icml10ed,
    publisher=icml10publ,
    pages={399--406},
    year=2010,
}
@article{grimes+rao:2005,
    author={D. B. Grimes and R. P.N. Rao},
    title={Bilinear Sparse Coding for Invariant Vision},
    journal={Neural Computation},
    year={2005},
    volume=17,
    number=1,
    pages={47 -- 73 },
}
@inproceedings{grosse+raina+kwong+ng:2007,
 author = {R. Grosse and R. Raina and H. Kwong and A. Y. Ng},
 title = {Shift-Invariant Sparse Coding for Audio Classification},
 crossref = {UAI:2007},
 pages = {}
}
@inproceedings{gutmann+hyvarinen:2009,
    author={M. Gutmann and A. Hyv{\"a}rinen},
    title={Learning Features by Contrasting Natural Images with Noise},
    booktitle={{ICANN}},
    year={2009},
    annote={
        Neural networks with a variety of hidden-layer non-linearities were
            trained to discriminate noise from real images.  The noise images
            were sampled with the same first- and second-order spatial
            statistics as the true images.
            They introduce a non-symmetric non-linearity with an optimized
            exponent, and it does well compared to sigmoid and tanh.
        Guillaume wrote to me that this is basically a quadratic unit, but I don't
        see the connection.
    },
}
%hhh % search target
@inproceedings{hadsell+chopra+lecun:2006,
    author={R. Hadsell and S. Chopra and Y. {LeCun}},
    title={Dimensionality Reduction by Learning an Invariant Mapping},
    booktitle=cvpr,
    year={2006},
    annote={Introduces contrastive algorithm for learning a non-linear embedding
        function such that similar points are grouped according to Euclidean
            Distance, and dissimilar ones are further away. This is the DrLIM
            algorithm.  The algorithm works on pairs of points.  The loss
            function is a decreasing quadratic function of the distance, down to
    0 at some 'margin' distance.  The loss function is an increasing quadratic
    function of the distance between similar points. Good results are shown on
    MNIST with artificial translations, and the airplane pictures from NORB.}
}
@article{halton:1960,
    title={On the Efficiency of Certain Quasi-Random Sequences of Points in
        Evaluating Multi-Dimensional Integrals},
    author={J. H. Halton},
    journal={Numerische Mathematik},
    publisher={Springer},
    volume={2},
    pages={84-90},
    year=1960,
    annote={Introduces the Halton low-discrepancy sequence.}
}
@article{hartman+keeler+kowalski:1990,
    author={E. J. Hartman and J. D. Keeler and J. M. Kowalski},
    title={Layered Neural Networks with {Gaussian} Hidden Units as Universal Approximations},
    journal=nc,
    year=1990,
    volume=2,
    number=2,
    pages={210-215},
    annote={See also (Hornik, 1989).}
}
@article{hausser+mel:2003,
    author = {M. Ha{\"u}sser and B. Mel},
    title = {Dendrites: Bug or Feature?},
    journal = {Current Opinion in Neurobiology},
    volume = {13},
    year = {2003},
    pages = {372-383},
}
@article{hedge+vanessen:2000,
    author={Hegd{\'e}, J. and Van Essen, D. C.},
    title={Selectivity for Complex Shapes in Primate
        Visual Area {V2}},
    journal={Journal of Neuroscience},
    volume={20},
    pages={1--6},
    year={2000},
    annote={}
}
@article{heeger:1992a,
    author={D. J. Heeger},
    title ={Normalization of Cell Responses in Cat Striate Cortex},
    journal ={Visual Neuroscience},
    volume={9},
    number={2},
    pages={181-198},
    year={1992},
}
@article{heeger:1993,
    author = {D. J. Heeger},
    title = {Modeling Simple Cell Direction Selectivity with Normalized, Half-Squared, Linear Operators},
    journal = {Journal of Neurophysiology},
    volume = {70}, 
    pages = {1885-1898},
    year = {1993}
}
@article{hinton:2002,
    author={G. E. Hinton},
    title={Training Products of Experts by Minimizing Contrastive Divergence},
    journal=nc,
    volume=14,
    pages={1771-1800},
    year=2002,
    annote={Introduces Contrastive Divergence (CD) for training
        product-of-experts models such as the RBM.}
}
@incollection{hinton:2007,
    author={G. E. Hinton},
    year=2007,
    title={To Recognize Shapes, First Learn to Generate Images},
    editor={P. Cisek and T. Drew and J. Kalaska},
    booktitle={Computational Neuroscience: Theoretical Insights into Brain Function},
    publisher={Elsevier},
}
@article{hinton:2010a,
    author={Hinton, G. E.},
    title={Learning to represent visual input},
    journal={Philosophical Transactions of the Royal Society, B},
    volume={365},
    pages={177-184},
    year=2010,
}
@techreport{hinton:2010b,
    author={G. E. Hinton},
    title={A Practical Guide to Training Restricted {Boltzmann} Machines},
    institution={University of Toronto},
    number={2010-003},
    year={2010},
    note={version 1},
    annote={Similar to Yann's tricks of the trade paper for neural networks, but for RBMs.}
}
@article{hinton+osindero+teh:2006,
    author={G. E. Hinton and S. Osindero and Y. Teh},
    title={A Fast Learning Algorithm for Deep Belief Nets},
    year=2006,
    journal=nc,
    volume=18,
    pages={1527-1554},
    annote={Stacking RBMs trained by Contrastive Divergence, pre-training.}
}
@article{hinton+salakhutdinov:2006,
    author={G. E. Hinton and R. R. Salakhutdinov},
    title={Reducing the Dimensionality of Data with Neural Networks},
    journal={Science},
    volume=313,
    number=5786,
    pages={504-507},
    month=jul,
    year=2006,
}
@article{hooker:1995,
    author = {Hooker, J.},
    affiliation = {Carnegie Mellon University Graduate School of
    Industrial Administration 15213 Pittsburgh PA USA},
    title = {Testing heuristics: We have it all wrong},
    journal = {Journal of Heuristics},
    publisher = {Springer Netherlands},
    issn = {1381-1231},
    keyword = {Computer Science},
    pages = {33-42},
    volume = {1},
    issue = {1},
    doi = {10.1007/BF02430364},
    note = {10.1007/BF02430364},
    year = {1995},
    annote={Distinguishes between scientific testing and competitive testing.
    Scientific testing is testing that gives insight into the strengths and
        weaknesses of an empirical heuristic algorithm.
    Competitive testing is benchmarking race sort of testing.  His first section
    outlines many evils of competitive testing.  Most significant to the ML and
    computer vision communities he points out the selection bias of benchmark
    problems - they are most often chosen because at least one algorithm (the first)
    did well on it. ``Worse than this, it is unclear that we would even be able to recognize
    a representative problem set if we had one.''}
}
@article{hornik:1989,
    author={K. Hornik},
    title={Multilayer Feedforward Networks are Universal Approximators},
    journal={Neural Networks},
    volume=2,
    number=5,
    pages={359-366},
    year=1989,
    doi={10.1016/0893-6080(89)90020-8},
    abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
    annote={See also (Hartman et al. 1990).}
}
@article{horton:2006,
    author={J. C. Horton},
    year=2006,
    title={Ocular Integration in Human Visual Cortex},
    journal={Canadian. Journal of Ophthalmology},
    volume={41},
    pages={584-593},
    annote={Has a nice picture of retinotopy in V1.}
}
@article{hoyer+hyvarinen:2000,
    author={P. O. Hoyer and A. Hyv{\"a}rinen},
    title={Independent Component Analysis Applied to Feature Extraction from
        Colour and Stereo Images},
    journal={Network: Computation in Neural Systems},
    volume={11},
    number={3},
    pages={191--210},
    year={2000},
    annote={Shows that the independent components of colour and stereo images
        are quite similar to the corresponding V1 receptive fields.}
}
@incollection{hsu+kakade+zhang:2008,
    author={Hsu, D. and Kakade, S. M. and Zhang, T.},
    title={A Spectral Algorithm for Learning Hidden {Markov} Models},
    booktitle=arxiv,
    pages={0811.4413}
}
@article{hubel+wiesel:1962,
    author={D. H. Hubel and T. N. Wiesel},
    title={Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex},
    journal={Journal of Physiology},
    year={1962},
    volume={160},
    number={1},
    pages={106--154},
    annote={}
}
@article{HubelWiesel1968,
    author={D. Hubel and T. N. Wiesel},
    title={Receptive Fields and Functional Architecture of Monkey Striate Cortex},
    journal={The Journal of physiology},
    volume={195},
    number={1},
    year={1968},
    pages={215-43},
}
@article{hung+kreiman+poggio+dicarlo:2005,
    author={C. Hung and G. Kreiman and T. Poggio and J. J. {DiCarlo}},
    title={Fast Readout of Object Identity from Macaque Inferior Temporal Cortex},
    journal={Science},
    volume={310},
    pages={863--866},
    year={2005},
    annote={Show that feedforward model rivals human performance in strictly
        feedforward image classification setting.}
}
@inproceedings{hurri+hyvarinen:2003,
    author={Hurri, J. and Hyv{\"a}rinen, A.},
    title={Temporal Coherence, Natural Image Sequences, and the Visual Cortex.},
    pages={141--148},
    crossref={NIPS15},
}
@phdthesis{hutter:2009,
    author={F. Hutter},
    title={Automated Configuration of Algorithms for Solving Hard Computational
        Problems},
    year=2009,
    school={University of British Columbia},
    annote={Evaluates model-based and model-free approaches to program
        configuration.  This work is relevant to the tuning of learning
            algorithm hyper-parameters.}
}
@inproceedings{hutter+hoos+leyton+brown:2011,
    author={F. Hutter and H. Hoos and K. Leyton-Brown},
    title={Sequential Model-Based Optimization for General Algorithm Configuration},
    booktitle={LION-5},
    note={Extended version as UBC Tech report TR-2010-10.},
    year={2011},
}
@inproceedings{hyndman+jepson+fleet:2007,
    title={Higher-Order Autoregressive Models for Dynamic Textures},
    author={Hyndman, M. and Jepson, A. D. and Fleet, D. J.},
    booktitle={British Machine Vision Conference ({BMVC})},
    year={2007},
    address={Warwick},
}
@article{hyvarinen:2009,
    author={A. Hyv{\"a}rinen},
    title={Statistical Models of Natural Images and Cortical Visual
        Representation},
    year=2009,
    journal={Topics in Cognitive Science},
    volume=2,
    pages={251--264},
    annote={Review of sparse coding and ICA models for V1.}
}
@article{hyvarinen+hoyer:2001,
    author={A. Hyv{\"a}rinen and P. O. Hoyer},
    title={A Two-Layer Sparse Coding Model Learns Simple and Complex Cell Receptive Fields and Topography from Natural Images},
    journal = {Vision Research},
    volume=41,
    number=18,
    pages={2413-2423},
    year={2001}
}
@article{hyvarinen+hoyer+inki:2001,
    author={A. Hyv{\"a}rinen and P. O. Hoyer and M. Inki},
    title={Topographic Independent Component Analysis},
    journal={Neural Computation},
    volume=13,
    number=7,
    pages={1527-1558},
    year=2001,
    annote={Introduces an extension of ICA. The dependencies of the estimated
        independent components are visualized as a topographic order. A new
            principle for topographic organization, based on higher-order
            statistics. Applied on image data, both topography and complex cell
    properties emerge.}
}
@book{hyvarinen+hurri+hoyer:2009,
    author={A. Hyv{\"a}rinen and J. Hurri and P. O. Hoyer},
    title={Natural Image Statistics--A probabilistic approach to early
        computational vision},
    publisher={Springer-Verlag},
    year={2009},
}
@book{hyvarinen+karhunen+oja:2001,
    author={A. Hyv{\"a}rinen and J. Karhunen and E. Oja},
    title={Independent Component Analysis},
    year={2001},
    publisher={John Wiley \& Sons},
    annote={}
}
@article{hyvarinen+koster:2007,
    author={A. Hyv{\"a}rinen and U. K{\"o}ster},
    title={Complex Cell Pooling and the Statistics of Natural Images},
    journal={Network: Computation in Neural Systems},
    volume=18,
    pages={81--100},
    year={2007},
    annote={Repeats subspace-ICA work with looser architecture constraints.
        Uses fully-connected units, experiments with linear filter exponent and
            pool size (number of filters). Finds that the optimal pool size is
            quite large (i.e. 16-32) and finds that the best exponent is 2.
    },
}
@article{hyvarinen+oja:2000,
    author={A. Hyv{\"a}rinen and E. Oja},
    title={Independent Component Analysis: Algorithms and
        Applications},
    journal={Neural Networks},
    volume=13,
    number={4--5},
    pages={411--430},
    year=2000,
    annote={ICA tutorial. The difference between PCA and ICA is that ICA
        maximizes the marginal non-gaussian-ness of each latent variable. The
            assumption is that when two non-gaussian latent variables (the
                    sources) are combined linearly (by mixing) then the result
            is close to Gaussian than either of the original sources. ICA seeks
    to undo this mixing by maximizing the un-gaussian-ness of each latent
    variable.  When the sources are indeed scalars being combined linearly, it
    seems like a good idea and algorithm. ICA is not a particular algorithm.
    Different techniques estimate non-Gaussian-ness, such as maximizing
    abs or square of kurtosis for given variance, and minimizing entropy.
    Several criteria are listed. This tutorial also describes the EVD-based
    whitening procedure (aka ZCA) in Section 5.2. Section 6 presents the FastICA
    algorithm.}
}
%iii  % search target
%jjj  % search target
@inproceedings{jenatton+obozinksi+bach:2010,
    author={R. Jenatton and G. Obozinski and F. Bach},
    title={Structured Sparse Principal Component Analysis},
    crossref={AISTATS:2010},
    pages={366-373},
    annote={
        Discusses the use of mixed L1 and L2 norms to express a prior of group
            sparsity on coefficients, and also a prior that dictionary elements
            should tend to have some standard non-zero patterns.
        Good notation and analysis of the runtime requirements.
        Not much discussion of when one should adopt such a prior, but perhaps
        that has been developed in related papers.  This one seems like part of
        a line of research on the subject coming out of Francis Bach's group.
    }
}
@article{jones+wang+sillito:2002,
    author={H. E. Jones and W. Wang and A. M. Sillito},
    title={Spatial Organization and Magnitude of Orientation Contrast
        Interactions in Primate {V1}.},
    journal={Journal of Neurophysiology},
    volume={88},
    pages={2796--2808},
    year=2002,
    annote={},
}
%kkkkk  % search target
@article{kapadia:1995,
    author = {M. K. Kapadia and M. Ito and C. D. Gilbert and G. Westheimer},
    title = {Improvement in Visual Sensitivity by Changes in Local Context: Parallel Studies in
        Human Observers and in {V1} of Alert Monkeys},
    journal = {Neuron},
    year = {1995},
    volume = {15},
    pages = {843-856},
}
@article{karklin+lewicki:2003,
    author={Y. Karklin and M. S. Lewicki},
    title={Learning Higher-order Structures in Natural Images},
    journal={Network: Computation in Neural Systems},
    volume=14,
    pages={483--499},
    year=2003,
    annote={Describes a two-layer ICA model.  The first layer is ICA. The second layer uses sparse coding to encode correlations in variance of the first layer units. Figure 8 is my favorite - it shows that if you color-code a large image by the maximally-active higher-order feature, that you get a nice segmentation of the image into abstract visual structure that groups textures.}
}
@article{karklin+lewicki:2005,
    author={Y. Karklin and M. S. Lewicki},
    title={A Hierarchical {Bayesian} Model for Learning Non-linear Statistical
        Regularities in Non-stationary Natural Signals},
    journal=nc,
    year=2005,
    volume=17,
    number=2,
    pages={397--423},
    annote={Seems like same model as 2003 paper, not sure what the contribution is compared to \cite{karklin+lewicki:2003}.}
}
@article{karklin+lewicki:2008,
    author={Y. Karklin and M. S. Lewicki},
    title={Emergence of Complex Cell Properties by Learning to Generalize in
        Natural Scenes},
    journal={Nature},
    year={2008},
    doi={10.1038/nature07481},
    month=nov,
    volume=457,
    pages={83--86},
    annote={Generalizes the hierarchical model of \cite{karklin+lewicki:2005} in
        allowing the topmost units to modulate not the {\em variance} of
            individual ICA coefficients, but rather than {\em covariance} among
            all coefficients.
        Shows that statistical models inferred from natural image patches
            develop common aspects of the neural response in complex cells in
            cortical area V1. They are highly tuned to the grating's
            orientation, but insensitive to its phase.
            TODO: understand masking results (Figure 3).}
}
@article{kastener+ungerleider:2000,
    author={Kastner, S. and Ungerleider, L. G.},
    year=2000,
    title={Mechanisms of Visual Attention in the Human Cortex},
    journal={Annual Reviews in Neuroscience},
    volume=23,
    pages={315--341},
}
@inproceedings{kavukcuoglu+ranzato+fergus+lecun:2009,
    author={K. Kavukcuoglu and M. Ranzato and R. Fergus and Y.
        {LeCun}},
    title={Learning Invariant Features through Topographic Filter Maps},
    annote={Uses encoder-decoder framework to learn filters.  Filters are
        arranged *logically* into a grid, and nearby filters in this grid are
            allowed to cooperate (via a sum-of-squares activation).  The result
            is the unsupervised learning of features that competes well with
            SIFT on CalTech and TinyImages.  The model is very similar to
            complex cell-like models.  One of the principles of pre-training in
            this model is that pooled features should compute *similar*
            (correlated?) features rather than independent (uncorrelated?) ones,
        but this is not explicit in the training criterion.
    },
    crossref={CVPR:2009},
}
@inproceedings{kavukcuoglu+etal:2010,
    author={K. Kavukcuoglu and P. Sermanet and Y. Boureau and K.
        Gregor and M. Mathieu and Y. {LeCun}},
    title={Learning Convolutional Feature Hierarchies for Visual Recognition},
    booktitle={{NIPS}},
    year={2010},
    annote={Describes the challenge when doing sparse coding when the
        generative model just matches the image size - there are boundary
        effects that are undesirable.  But why not just generate a region
        larger than the image and only apply the reconstruction error to the
        subregion corresponding to the actual image?
        This paper also includes the 'learning to encode' work that Karol presented
        at Snowbird last year.
    }
}
@inproceedings{kingma+lecun:2010,
    author={D. P. Kingma and Y. {LeCun}},
    title={Regularized estimation of image statistics by Score Matching},
    booktitle={{NIPS}},
    year={2010},
    annote={This is very similar to Pascal's score matching for denoising, not
    sure what the difference is.},
}
@article{kirkpatrick+gelatt+vecchi:1983,
    title = {Optimization by Simulated Annealing},
    author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
    journal = {Science},
    volume = {220},
    number = {4598},
    series = {New Series},
    pages = {671-680},
    year = {1983},
    publisher = {American Association for the Advancement of Science},
    annote={Introduces the technique of simulated annealing for global
        optimization.}
}
@article{kobatake+tanaka:1994,
    author={Kobatake, E. and Tanaka, K.},
    title={Neuronal Selectivities to Complex Object Features
        in the Ventral Visual Pathway of the Macaque Cerebral Cortex},
    journal={Journal of  Neurophysiology},
    volume=71,
    pages={856--867},
    year={1994},
    annote={}
}
@article{kohonen:1996,
    author={T. Kohonen},
    year=1996,
    title={Emergence of Invariant-Feature Detectors in the Adaptive-Subspace
        Self-Organizing Map},
    journal={Biological Cybernetics},
    volume=75,
    number=4,
    doi={10.1007/s004220050295},
    pages={281--291},
    annote={Presents the ``Adaptive Subspace Self-Organizing Map'' ASSOM
        algorithm, which is like the SOM, but for learning linear transforms.
    This is one of the first 'quadratic unit' models, as far as I know.}
}
@article{kording+etal:2004,
    author={K. P. K{\"o}rding and C. Kayser and W.
        Einh{\"a}user and P. K{\"o}nig},
    title = "How Are Complex Cell Properties Adapted to the Statistics of
        Natural Stimuli?",
    year = 2004,
    journal = "Journal of Neurophysiology",
    volume = 91,
    pages = {206--212},
    annote = {
        Computer simulations show that two principles suffice to explain the
            responses of V1 complex cells: the maximization of temporal
            correlation in each cell, and the minimization of correlation
            between cells.
            They optimize the parameters of an Adelson and Bergen model by
            gradient descent on this cost function, considering 32x32 pixel
            image patches from adjacent video frames.
            The response of model neurons is evaluated on drifting gratings.
            The neurons learn to be localized, and selective to orientation and
            spatial frequency.  Most neurons learn filters that are offset by 90 degrees
            to implement translation invariance.
            Authors argue that sparsity explains simple cells, but stability
            over time explains complex cells.
    }
}
@article{kording+konig:2001,
    author = {K. P. K{\"o}rding and P. K{\"o}nig},
    title = {Supervised and Unsupervised Learning with Two Sites of Synaptic Integration},
    journal = {Computational Neuroscience},
    year = {2001},
    volume = {11},
    pages = {207-215}
}
@inproceedings{koster+etal:2009,
    author={U. K{\"o}ster and J. Lindgren and M. Gutmann and A. Hyv{\"a}rinen},
    title={Learning Natural Image Structure with a Horizontal Product Model},
    booktitle={Proc. of the International Conference on
        Independent Component Analysis and Blind Source Separation (ICA2009)},
    location={Paraty, Brazil},
    year={2009},
    tag={TransformationLearning},
    annote={}
}
@article{kouh+poggio:2008,
    author={M. M. Kouh and T. T. Poggio},
    title={A Canonical Neural Circuit for Cortical Nonlinear Operations},
    journal={Neural Computation},
    volume={20},
    number={6},
    year={2008},
    pages={1427-51},
}
@article{kreiman:2008,
    author = {G. Kreiman},
    title = {Biological Object Recognition},
    journal = {Scholarpedia},
    volume=3,
    number=6,
    pages=2667,
    year=2008,
    doi={10.4249/scholarpedia.2667},
}
@article{kreiman+etal:2006,
    author={G. Kreiman and C. P. Hung and A. Kraskov and R. Q. Quiroga and T. Poggio and J. J. {DiCarlo}},
    title={Object Selectivity of Local Field Potentials and Spikes in the Macaque Inferior Temporal Cortex},
    journal={Neuron},
    volume=49,
    pages={433--445},
    year=2006,
    publisher={Elsevier},
    doi={10.1016/j.neuron.2005.12.019},
}
@techreport{krizhevsky:2009,
    title={Learning Multiple Layers of Features from Tiny Images},
    author={A. Krizhevsky},
    year={2009},
    annote={GRBM results on {CIFAR}-10},
    institution={University of Toronto},
}
@techreport{krizhevsky:2010,
    title={Convolutional Deep Belief Networks on {CIFAR}-10},
    author={A. Krizhevsky},
    year={2010},
    annote={Convolutional Rectified-linear unit GRBM results on CIFAR-10},
    institution={University of Toronto},
}
@inproceedings{kumar+packer+koller:2010,
    author={M. P. Kumar and B. Packer and D. Koller},
    title={Self-Paced Learning for Latent Variable Models},
    booktitle={{NIPS}},
    year={2010},
    annote={
        Follows up on curriculum learning (Bengio et al., 2009) with the idea
        that the curriculum should be inferred from the trouble the model is
        having with the examples.
        Trouble is defined as 'has difficulty classifying correctly'.
        Introduces a reformulation of the learning problem of the "Latent SSVM"
        model in which binary variables are introduced for each element of the
        training set.  At every iteration the model can either make a prediction
        for an example, or pay fixed price (1/K in paper).  The price of
        skipping examples is annealed *by hand* during learning so that at
        the beginning about half of the dataset is used, and at the end
        all of the dataset it used.  Results are ok but neither thorough
        nor impressive (e.g. only 4 binary decisions regarding MNIST digits
        are shown - why?).
        TODO: Compare with another paper here: "Trading off Mistakes and
        Don't-Know Predictions"
    }
}
%lll % search target
@article{langford+li+zhang:2009,
    title={Sparse Online Learning via Truncated Gradient},
    author={J. Langford and L. Li and T. Zhang},
    pages={777--801},
    journal = {JMLR},
    volume = {10},
    month="March",
    year={2009},
}
@inproceedings{langford+salakhutdinov+zhang:2009,
    author={J. Langford and R. Salakhutdinov and T. Zhang},
    title={Learning Nonlinear Dynamic Models},
    booktitle=icml09,
    editor=icml09ed,
    publisher=icml09publ,
    year={2009},
    annote={Presents a novel interpretation that reintroduces recurrent neural networks as a means of modeling timeseries data.
        Training is done by backprop through time.
        Experiments show that this method outperforms HMM in modeling motion capture data used in Taylor et al. 2006.
        Long-range predictions (exponential) are to be made using a hierarchical set of temporal predictors, but no experiments demonstrate this.
    }
}
@inproceedings{larochelle+bengio:2008,
    author = "H. Larochelle and Y. Bengio",
    title = {Classification using Discriminative Restricted {Boltzmann} Machines},
    pages = {536--543},
    crossref={ICML:2008},
    annote={}
}
@INPROCEEDINGS{larochelle+etal:2007,
    author = {Larochelle, H. and Erhan, D. and Courville, A. and Bergstra, J. and Bengio, Y.},
     title = {An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation},
     pages = {473--480},
     crossref = {ICML:2007},
  abstract = {Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit  recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.},
}
@incollection{larochelle+hinton:2010,
    title = {Learning to Combine Foveal Glimpses with a Third-order {Boltzmann} Machine},
    author = {H. Larochelle and G. E. Hinton},
    crossref={NIPS23},
    pages = {1243--1251},
}
@inproceedings{lazebnik+schmid+ponce:2006,
    author={Lazebnik, S. and Schmid, C. and Ponce, J.},
    year=2006,
    title={Beyond Bags of Features: Spatial Pyramid Matching for Recognizing
        Natural Scene Categories},
    booktitle=cvpr06,
    annote={Describes Pyramid Histogram of Words (PHOW) image features (also known as the
            Spatial Pyramid).}
}
@inproceedings{le+ngiam+chen+chia+koh+ng:2010,
    title={Tiled Convolutional Neural Networks},
    author={Q. V. Le and J. Ngiam and Z. Chen and D. Chia and P. W. Koh and A. Y. Ng},
    booktitle={{NIPS}},
    year={2010},
    annote={
        Uses Topographic ICA to train a so-called "tiled" convolutional model in which
        filters are shifted by K pixels instead of 1 pixel, and started at
        position 0 <= (x,y) < (K,K).
        The results are good, given for NORB and CIFAR-10, where they are just
        above state of art for NORB (96\%) and just below state of art for
        CIFAR-10 (73\%).
        There are some technical details given for how and why filters are orthogonalized.
        (TODO)
        I would like to ask Yann LeCun or Koray or Marc'Aurelio about the tiling
        idea - because it seems to me that they already did this.  Not sure what
        else is new here other than a combination of criterion and model that
        gives good results on these datasets.
    }
}
@inproceedings{lecun:1985,
    author={Y. {LeCun}},
    title={Une Procedure d'Apprentissage pour Reseau a Seuil Assymetrique},
    booktitle={Proc. of Cognitiva 85: A la Frontiere de l'Intelligence Artificielle des Sciences de la Connaissance des Neurosciences},
    pages={599--604},
    year=1985,
    address={Paris},
}
@misc{lecun-MNIST,
    author={Y. {LeCun}},
    title={The {MNIST} Database of Handwritten Digits},
    howpublished={{\url{http://yann.lecun.com/exdb/mnist/}}},
    year={1998-},
    annote={This is supposed to refer to the website.
        The dataset is introduced in~\cite{lecun+bottou+bengio+haffner:1998}.}
}
@article{lecun+etal:1989,
    author={Y. {LeCun} and B. Boser and J. S. Denker and D. Henderson and R. E.
        Howard and W. Hubbard and L. D. Jackel},
    title={Backpropagation Applied to Handwritten Zip Code Recognition},
    journal=nc,
    volume=1,
    number=4,
    pages={541-551},
    year=1989,
    annote={Introduces convolutional neural network.}
}
@article{lecun+bottou+bengio+haffner:1998,
    author =    {{LeCun}, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
    title =     {Gradient-Based Learning Applied to Document Recognition},
    journal =   {Proceedings of the IEEE},
    month =         {November},
    volume =        86,
    number =        11,
    pages =  {2278--2324},
    year =      1998,
    annote = {This is the standard citation for MNIST and the LeNet5 architecture.}
}
@incollection{lecun+bottou+orr+muller:1998,
    author={Y. {LeCun} and L. Bottou and G. Orr and K. Muller},
    title={Efficient Backprop},
    booktitle={Neural Networks: Tricks of the Trade},
    year={1998},
    editor={G. Orr and K. Muller},
    publisher={Springer},
}
@InProceedings{lecun+huang+bottou:2004,
  author =       "Y. {LeCun} and F. Huang and L. Bottou",
  booktitle =    cvpr04,
  title =        "Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting",
  volume = {2},
  year = "2004",
  pages = {97-104},
  doi = {10.1109/CVPR.2004.144},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  annote={Introduces NORB dataset.}
}
@article{lecuyer+blouin+couture:1993,
    author={P. L'Ecuyer and F. Blouin and R. Couture},
    title={A Search for Good Multiple Recursive Generators},
    journal={ACM Transactions on Modeling and Computer Simulation},
    volume=3,
    pages={87--98},
    year=1993,
    annote={Defines the MRG generator that was implemented in Theano to have random numbers on the GPU.}
}
@InCollection{lee+battle+raina+ng:2008,
    title = "Efficient Sparse Coding Algorithms",
    author = "H. Lee and A. Battle and R. Raina and A. Y. Ng",
    booktitle = NIPS19,
    editor = NIPS19ed,
    publisher= NIPS19publ,
    pages = "801--808",
    year = "2008"
}
%was Lee2008
@inproceedings{lee+ekanadham+ng:2008,
    title={Sparse Deep Belief Net Model for Visual Area {V2}},
    author={H. Lee and C. Ekanadham and A. Y. Ng},
    booktitle = nips20,
    editor = nips20ed,
    publisher = nips20publ,
    address = {Cambridge, MA},
    pages = {873--880},
    year = {2008}
}
@inproceedings{lee+grosse+ranganath+ng:2009,
    author={H. Lee and R. Grosse and R. Ranganath and A. Y. Ng},
    title={Convolutional Deep Belief Networks for Scalable Unsupervised Learning
        of Hierarchical Representations},
    booktitle=icml09,
    editor=icml09ed,
    publisher=icml09publ,
    year={2009},
    annote={Introduces Convolutional Deep Belief Network, which is an
        adaptation of LeNet5 to support bottom-up and top-down inference, as
            well as learning by Contrastive Divergence.
            Introduces "probabilistic max-pooling", a variation on max-pooling
            that carries a sensible generative interpretation: a pooling unit is
            on iff any of its inputs is on, and at most one of its inputs can be
            on.
            Classification accuracies reported: 57.7\% on caltech-101 using 15
            training images per class; 65.4\% on caltech-101 using 30 training
            images per class; 0.8\% on MNIST.
            They tested the generative modeling ability by zero-ing out testing
            images, and then filling them in using either second layer (not so good) or
            repeated Gibbs sampling throughout the model (better).
    }
}
@inproceedings{lee+largman+pham+ng:2009,
    author={H. Lee and Y. Largman and P. Pham and A. Y. Ng},
    title={Unsupervised Feature Learning for Audio Classification Using
        Convolutional Deep Belief Networks},
    booktitle={{NIPS} 2009},
    year={2009},
    annote={
        Extends Lee et al. ICML2009 to  work on audio - speech and music.
        It is a convolution through time, not frequency.
            Speech tasks: speaker id., gender id., phone classification.
            Music tasks: genre, artist.
        On several tasks, second-layer features outperform first-layer features.
    }
}
@incollection{leroux+manzagol+bengio:2008,
 title = {Topmoumoute Online Natural Gradient Algorithm},
 author = {N. {Le Roux} and P. Manzagol and Y. Bengio},
 booktitle = {Advances in Neural Information Processing Systems 20},
 editor = {J. C. Platt and D. Koller and Y. Singer and S. Roweis},
 publisher = {MIT Press},
 address = {Cambridge, MA},
 pages = {849--856},
 year = {2008}
}
@incollection{lettvin+etal:1968,
    author={J. Y. Lettvin and H. R. Maturana and W. S. {McCulloch} and W. H.
        Pitts},
    title={What the Frog's Eye Tells the Frog's Brain},
    chapter=7,
    booktitle={The Mind: Biological Approaches to its Functions},
    editor={W. C. Corning and M. Balaban},
    pages={233--258},
    year={1968},
    annote={Frogs seem to respond only to motion, not to still images. The eyes
        do not move except to stabilize the image perceived by a moving frog. The
        retinal ganglion cells of the frog are studied. Four kinds of fibers
            are characterized. (1) sustained contrast detection (i.e. edge
                    detection). (2) net convexity detectors (i.e. fly
                        detectors!). (3) moving edge detectors. (4) net dimming
    detection (light-off detector).}
}
@article{leuba+kraftsik:1994,
    author={G. Leuba and R. Kraftsik},
    title={Changes in Volume, Surface Estimate, Three-Dimensional Shape and Total Number of Neurons of the Human Primary Visual Cortex from Midgestation Until Old Age},
    journal={Anatomy and Embryology},
    volume=190,
    year=1994,
    number=4,
    pages={351--366},
    doi={10.1007/BF00187293},
    annote={Estimates the number of V1 neurons in right hemisphere at roughly
        140 million.}
}
@article{li+dicarlo:2008,
    author={Li, N. and {DiCarlo}, J. J.},
    title={Unsupervised Natural Experience Rapidly Alters
        Invariant Object Representation in Visual Cortex},
    journal={Science},
    volume=321,
    number={5895},
    pages={1502--1507},
    year={2008},
    annote={Shows that just 1 hour of unsupervised experience is enough for IT neurons to start grouping novel artificial patterns in temporal correlation.},
}
@inproceedings{liang+klein:2009,
    author={P. Liang and D. Klein},
    title={Online EM for Unsupervised Models},
    booktitle={Human Language Technologies ({HLT})},
    pages={611-619},
    year={2009},
    month={June},
    address={Boulder, Colorado},
    annote={Reminder that EM can be performed as an online algorithm.  Offers an empirical comparison of Incremental EM (Neal and Hinton 1998) and Stepwise EM (Sato and Ishii 2000, Cappe and Moulines 2009) on NLP tasks.  Stepwise EM is compared to SGD, Incremental EM compared to exponentiated gradient.  Stepwise EM is found to be faster than both batch and incremental EM, and to converge to better solutions as well.}
}
@article{lodhi+saunders+shawe-taylor+cristianini+watkins:2002,
    title={Text Classification using String Kernels},
    author= {H. Lodhi and C. Saunders and J. Shawe-Taylor and N.
        Cristianini and C. Watkins},
    journal={Journal of Machine Learning Research},
    volume=2,
    month=Feb,
    pages={419-444},
    year=2002,
    annote={Introduces a DP algorithm for rapid evaluation of a substring-based
        distance between strings.  Shows that this is a valid kernel and posts
        results with SVM classifier.}
}
@incollection{loosli+canu+bottou:2006,
    author={G. Loosli and S. Canu and L. Bottou},
    title={Training Invariant Support Vector Machines using Selective Sampling},
    pages = {301-320},
    editor = {Bottou, L. and Chapelle, O. and {DeCoste}, D. and Weston, J.},
    booktitle = {Large Scale Kernel Machines},
    publisher = {MIT Press},
    address = {Cambridge, MA.},
    year = {2007},
    annote={Introduces InfiniteMNIST, a dataset of 8 million points derived from
        MNIST by applying random sub-pixel level deformations, and pixel-level
            translations.  Evaluates a strategy for online-learning of SVMs to
            take advantage of data-driven invariance. Shows that online SVM can
    handle really large datasets. Using larger dataset yields MNIST error rate
    of 0.67\%, after 8 days of training.}
}
@inproceedings{lowe:1999,
    author={D. G. Lowe},
    year=1999,
    title={Object Recognition from Local Scale-Invariant Features},
    booktitle={Proceedings of the International Conference on Computer
        Vision 2 ({ICCV})},
    pages={1150--1157},
    doi={10.1109/ICCV.1999.790410},
    annote={Describes SIFT feature.}
}
@article{lowe:2004,
    author={D. G. Lowe},
    title={Distinctive image features from scale-invariant keypoints},
    journal={International Journal of Computer Vision},
    volume=60,
    number=2,
    year=2004,
    pages={91-110},
    annote={Describes the SIFT feature.}
}
@inproceedings{lyu+simoncelli:2007spie,
    author={S. Lyu and E. P. Simoncelli},
    title = {Statistically and Perceptually Motivated Nonlinear Image
        Representation},
    doi={10.1117/12.729071},
    pages= "67--91",
    crossref={spie:2007},
    annote={},
}
@inproceedings{lyu+simoncelli:2007nips,
    author={S. Lyu and E. P. Simoncelli},
    title={Statistical Modeling of Images with Fields of {Gaussian} Scale
        Mixtures},
    crossref={NIPS19},
    annote={}
}
@inproceedings{lyu+simoncelli:2008cvpr,
    author={S. Lyu and E. P. Simoncelli},
    title = {Nonlinear Image Representation Using Divisive Normalization},
    pages={1--8},
    crossref={CVPR:2008},
    doi={10.1109/CVPR.2008.4587821},
    annote={}
}
@INPROCEEDINGS{lyu+simoncelli:2009nips,
    TITLE= "Reducing Statistical Dependencies in Natural Signals using Radial {Gaussianization}",
    AUTHOR= "S. Lyu and E. P. Simoncelli",
    PAGES= "1009--1016",
    crossref={NIPS21},
    annote={}
}
@ARTICLE{lyu+simoncelli:2009pami,
    AUTHOR= "S. Lyu and E. P. Simoncelli",
    TITLE= "Modeling Multiscale Subbands of Photographic Images with Fields of {Gaussian} Scale Mixtures",
    JOURNAL= pami,
    VOLUME= 31,
    NUMBER= 4,
    PAGES= "693--706",
    MONTH= "Apr",
    YEAR= 2009,
    DOI= "10.1109/TPAMI.2008.107",
    PUBLISHER= "IEEE Computer Society",
    ADDRESS= "Los Alamitos, CA",
}
@ARTICLE{lyu+simoncelli:2009nc,
    TITLE= "Nonlinear Extraction of `Independent Components' of Natural Images using Radial {Gaussianization}",
    AUTHOR= "S. Lyu and E. P. Simoncelli",
    JOURNAL= nc,
    VOLUME= 21,
    NUMBER= 6,
    PAGES= "1485--1519",
    MONTH= "Jun",
    YEAR= "2009",
    DOI="10.1162/neco.2009.04-08-773",
    annote={},
}
%mmm % search target
@article{maes:2009,
    title={Nieme: Large-Scale Energy-Based Models},
    author={F. Maes},
    pages={743--746},
    journal = jmlr,
    volume = {10},
    month="March",
    year={2009},
}
@article{mairal+bach+ponce+sapiro:2010,
    author = {J. Mairal and F. Bach and J. Ponce and G. Sapiro},
    title={Online Learning for Matrix Factorization and Sparse Coding},
    journal=jmlr,
    volume=11,
    pages={10-60},
    year={2010},
    annote={Presents a fast online algorithm for L1 sparse coding.},
}
@inproceedings{martens:2010,
    title={Deep Learning via Hessian-free Optimization},
    author={J. Martens},
    booktitle=icml10,
    editor=icml10ed,
    publisher=icml10publ,
    year=2010,
    pages={735--742},
}
@article{maunsell+treue:2006,
    author={J. H. R. Maunsell and S. Treue},
    year=2006,
    title={Feature-based Attention in Visual Cortex},
    journal={Trends in Neuroscience},
    volume=29,
    pages={317-322},
}
@article{mcadams:2000,
    author = {C. J. McAdams and J. H. Maunsell},
    title = {Attention to Both Space and Feature Modulates Neuronal Responses in Macaque Area {V4}},
    journal = {Neurophysiology},
    year = {2000},
    volume = {83},
    pages = {1751-1755},
}
@article{mccullough:1965,
    author={McCollough, C.},
    year=1965,
    title={Adaptation of Edge-detectors in the Human Visual System},
    journal={Science},
    volume=149,
    pages={1115--1116},
    annote = {Describes McCullough Effect.}
}
@article{mcfee+lanckriet:2011,
    title={Learning Multi-modal Similarity},
    author={B. McFee, G. Lanckriet},
    journal=JMLR,
    month=feb,
    volume=12,
    pages={491--523},
    year={2011},
    annote={TODO ``We present a novel multiple kernel learning technique for integrating heterogeneous data into a single, unified similarity space.''}
}
@article{memisevic+hinton:2010,
    author={Memisevic, R. and Hinton, G. E.},
    title={Learning to Represent Spatial Transformations with Factored
        Higher-order {Boltzmann} Machines},
    journal={Neural Computation},
    volume={22},
    pages={1473-1492},
    year={2010},
}
@book{minsky+papert:1969,
    author={M. L. Minksy and S. A. Papert},
    title={Perceptrons},
    publisher={MIT Press},
    address={Cambridge, MA},
    year=1969,
}
@Article{mitchell+beauchamp:1988,
  author =       {T. J. Mitchell and J. J. Beauchamp},
  title =        {Bayesian Variable Selection in Linear Regression},
  journal =      {J. Amer. Statistical Assoc.},
  Year =         {1988},
  volume =    {83},
  number =    {404},
  pages =     {1023-1032},
  annote = {Introduces ``Spike and Slab'' term.}
}
@inproceedings{mobahi+collobert+weston:2009,
    author={M. Mobahi and R. Collobert and J. Weston},
    title={Deep Learning from Temporal Coherence in Video},
    booktitle=icml09,
    editor=icml09ed,
    publisher=icml09publ,
    year={2009},
    annote={
        Introduces a temporal coherence criterion (L1 on the feature differences) as a regularization of the LeNet architecture.
        Temporal coherence enforced only at the next-to-last layer of the deep net.
        Evaluation is done using COIL100, a COIL100-like homemade DB, a homemade db of pictures of rubber animals similar to Small-NORB, and the ORL face dataset.
        There is mention of the Spin-Glass MRF (***).
    }
}
@article{movshon+thompson+tolhurst:1978,
    author={Movshon, J. A. and Thompson, I. D. and Tolhurst, D. J.},
    title={Receptive Field Organization of Complex Cells in the Cat's Striate
        Cortex},
    journal={Journal of Physiology},
    volume=283,
    pages={79--99},
    year={1978},
    annote={}
}
@article{mutch+lowe:2008,
    author={Mutch, J. and Lowe, D. G.},
    year=2008,
    title={Object Class Recognition and Localization Using Sparse Features with
        Limited Receptive Fields},
    journal={International Journal of Computer Vision ({IJCV})},
    volume=80,
    pages={45--57},
    annote={Describes Sparse Localized Features (SLF) for images.}
}
%nnn % search target
@incollection{nair+hinton:2009,
 title = {Implicit Mixtures of Restricted {Boltzmann} Machines},
 author = {V. Nair and G. E. Hinton},
 pages = {1145--1152},
 crossref={NIPS21},
}
@InProceedings{nair+hinton:2010,
  author =       {V. Nair and G. E. Hinton},
  title =        {Rectified Linear Units Improve Restricted {B}oltzmann Machines},
  pages =     {807--814},
  crossref={ICML:2010},
}
@article{NakaRushton1966a,
    author={K. I. Naka and W. A. Rushton},
    title={S-potentials from Luminosity Units in the Retina of Fish (Cyprinidae)},
    journal={The Journal of physiology},
    volume={185},
    number={3},
    year={1966},
    pages={587-99},
}
@article{nareyek:2001,
    author={A. Nareyek},
    title={Choosing search heuristics by non-stationary reinforcement learning},
    journal={Applied Optimization},
    year=2003,
    volume=86,
    pages={523-544},
    publisher={Kluwer Academic Publishers},
    address={Netherlands},
}
@PhdThesis{neal:1994,
  author =       "R. M. Neal",
  title =        "Bayesian Learning for Neural Networks",
  school =       "Dept. of Computer Science, University of Toronto",
  year =         "1994",
  annote = {Introduces Hybrid Monte Carlo (HMC) sampling for energy models, among other things.}
}
@InCollection{neal:1998,
    author =       "R. M. Neal",
    editor =       "C. M. Bishop",
    booktitle =    "Neural Networks and Machine Learning",
    title =        "Assessing relevance determination methods using {DELVE}",
    publisher =    "Springer-Verlag",
    pages =        "97--129",
    year =         1998,
    annote = {Introduces Automatic Relevance Determination (ARD).}
}

@article{nelder+mead:1965,
    author={Nelder, J. A. and Mead, R.},
    title={A simplex method for function minimization},
    journal={The Computer Journal},
    volume=7,
    year=1965,
    pages={308--313},
    annote={Describes Nelder-Mead Simplex algorithm for derivative free
        optimization of a unimodal function.},
}
@article{NykampRingach2002,
    author={D. Q. Nykamp and D. L. Ringach},
    title ={Full Identification of a Linear-Nonlinear System via Cross-Correlation Analysis},
    journal = {Journal of Vision},
    volume={2},
    pages={1--11},
    year={2002},
}
%ooo  %search target
@article{oliphant:2007,
    author={T. E. Oliphant},
    title={Python for Scientific Computing},
    journal = {Computing in Science \& Engineering},
    volume=9,
    pages=10,
    year=2007,
    annote={This is the reference for NumPy.}
}
@inproceedings{olshausen:2003,
    author  = {B. A. Olshausen},
    title   = {Learning sparse, overcomplete representations of time-varying natural images},
    crossref = {ICPC:2003},
    pages = {},
}
@article{olshausen+anderson+vanessen:1993,
    author={B. A. Olshausen and C. H. Anderson and D. C. {Van Essen}},
    title={A Neurobiological Model of Visual Attention and Invariant Pattern Recognition Based on Dynamical Routing of Information},
    journal={Neuroscience},
    month=nov,
    year=1993,
    volume=13,
    number=11,
    pages={4700--4719},
}
@inproceedings{olshausen+etal:2007,
    author={B. A. Olshausen and C. Cadieu and J. Culpepper and D.
        K. Warland},
    title={Bilinear Models of Natural Images},
    crossref={spie:2007},
    annote={Introduces a sparse coding approach to learning transformations,
    which is shown to be similar to the sort of bilinear modeling advanced by
        \cite{grimes+rao:2005} and \cite{tenenbaum+freeman:2000}.
        Olshausen is interested in bilinear models also known as routing models,
    since apparently his PhD dissertation which was about dynamic routing.
        One of the terms in the bilinear decomposition is penalized with a
        slowness criterion, which seems similar to Cadieu's later
        articles on phase variables in the sparse coding of motion.
    }
}
@article{olshausen+field:1996,
    author = {B. A. Olshausen and D. J. Field},
    title = {Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images},
    journal = {Nature},
    volume = {381},
    pages = {607-691},
    year = {1996},
  annote = {TODO: diff with 1997?}
}
@Article{olshausen+field:1997,
  author =       "B. A. Olshausen and D. J. Field",
  title =        "Sparse Coding with an Overcomplete Basis Set: a Strategy Employed by {V}1?",
  journal =      "Vision Research",
  volume =       "37",
  pages =        "3311--3325",
  year =         "1997",
  month = dec,
  annote = {TODO: diff with 1996?}
}
@article{olshausen+field:2005,
    author = {B. A. Olshausen and D. J. Field},
    title = {How Close are We to Understanding {V1}?},
    journal = nc,
    volume = {17},
    pages = {1665-1699},
    year = {2005},
    annote={Epic review of failures of standard V1 models to account for what happens when subjects are tested on natural stimuli rather than bars and gratings.  First half of article reviews 5 major problems: biased sampling of neurons, biased stimuli to test them, oversimplified theories, unexplained effects of interdependence and context, and the fact that current models only explain about 15\% of the variance in what can be measured about V1. The second half of the article outlines 5 directions where the authors would like to see more research: studying many neurons at once, something about sparse coding, contour integration, surface representation, top-down feedback and disambiguation, and dynamic routing (bilinear models).}
}
@book{Oreilly2000,
    author={R. C. O'Reilly and Y. Munakata},
    title ={Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain},
    year ={2000},
    publisher={{MIT} Press},
    address = {Cambridge},
}
@Article{osinder+welling+hinton:2006,
  author =       {S. Osindero and M. Welling and G. E. Hinton},
  title =        {Topographic Product Models applied to Natural Scene Statistics},
  journal =      {Neural Computation},
  year =         {2006},
  volume =    {18},
  pages =     {344-381},
}
%ppp  %search target
@incollection{palmer:1983,
    author={Palmer, S. E.},
    title={The Psychology of Perceptual Organization: a Transformational Approach},
    booktitle={Human and Machine Vision},
    editor={Beck, J. and Hope, B. and Rosenfeld, A.},
    pages={269--339},
    publisher={Academic},
    address={Orlando},
    annote = {Referenced by \cite{olshausen+anderson+vanessen:1993} for the idea that ``the process of attending to an object places it in some canonical, or object-based reference frame''.}
}
@article{pasupathy+connor:2001,
    author={Pasupathy, A. and Connor, C. E.},
    title={Shape Representation in Area {V4}:
        Position-specific Tuning for Boundary Conformation},
    journal={Journal of Neurophysiology},
    volume={86},
    pages={2505--2519},
    year=2001,
    annote={}
}
@InProceedings{perpinan+hinton:2005,
  author =       "M. A. Carreira-Perpi{\~{n}}an and G. E. Hinton",
  editor =       aistats05ed,
  booktitle =    aistats05,
  title =        "On Contrastive Divergence Learning",
  publisher =    aistats05publ,
  pages =        "33--40",
  year =         "2005",
}
@article{pesaran+nelson+andersen:2006,
    author={Pesaran, B. and Nelson, M. J. and Andersen, R. A.},
    year=2006,
    title={Dorsal Premotor Neurons Encode the Relative Position of the Hand, Eye, and Goal During Reach Planning},
    journal={Neuron},
    volume=51,
    number=1,
    pages={125-34}
}
@article{peters+payne+budd:1994,
    author={A. Peters and B. R. Payne and J. Budd},
    title={A Numerical Analysis of the Geniculocortical Input to Striate Cortex in the Monkey},
    journal={Cerebral Cortex},
    volume=4,
    pages={215--229},
    year=1994,
    annote = {Referenced by \cite{olshausen+field:2005} because it shows how relatively little input V1 receives from LGN compared to from itself and other regions.}
}
@article{pinto+cox+dicarlo:2008,
    author={N. Pinto and D. D. Cox and J. J. {DiCarlo}},
    title={Why is Real-World Visual Object Recognition Hard?},
    journal={PLoS Computational Biology},
    volume=4,
    number=1,
    doi={doi:10.1371/journal.pcbi.0040027},
    year={2008},
    annote={Presents a classification model that is V1-like elements with a
        linear SVM classifier on top.  This model achieves results on
            Caltech-101 and Caltech-256, but fails to recognize objects that
            have been super-imposed on a white-noise background.  The conclusion
            is that backgrounds in photographs are being used by the model.
    The paper includes instructions for how to implement a V1-like
    hidden layer of simple cells, including contrast gain-normalization in both
    the input (retinal ganglion) and output (simple cell response).
    This paper points out the problem of sample bias in using un-controlled
    data for high-dimensional problems with many factors of variation.  It is
    not possible to gather enough data to make systematic tests.  These
    difficulties are very similar to the motivation behind the BabyAI-related
    projects in our lab.}
}
@article{pinto+doukhan+dicarlo+cox:2009,
  author = {Pinto, N. and Doukhan, D. and {DiCarlo}, J. J. and Cox, D. D.},
  journal = {PLoS Comput Biol},
  publisher = {Public Library of Science},
  title = {A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation},
  year = {2009},
  month = {11},
  volume = {5},
  pages = {e1000579},
  number = {11},
}
@article{poczos:2009,
    author={B. P{\'o}czos, A. L{\"o}rincz},
    title={Identification of Recurrent Neural Networks by Bayesian Interrogation
        Techniques},
    journal = jmlr,
    year=2009,
    volume=10,
    month=February,
    pages={515--554},
}
@article{poirazi:2003,
    author={P. Poirazi and T. Brannon and  W. Mel},
    title ={Arithmetic of Subthreshold Synaptic Summation in a Model {CA1} Pyramidal Cell},
    journal={Neuron},
    volume={37},
    pages={977-987},
    year={2003},
}
@book{popper:1934,
    title={Logik der Forschung},
    author={K. Popper},
    publisher={Julius Springer Verlag},
    address={Vienna},
    year=1935,
    annote={Introduces Karl Popper's doctrine of falsifiability as the foundation of good scientific theory.}
}
@article{powell:1994,
    author={M. J. D. Powell},
    title={A direct search optimization method that models the objective and constraint functions by linear interpolation},
    journal={Advances in Optimization and Numerical Analysis},
    editor={S. Gomez and J-P. Hennart},
    publisher={Kluwer Academic},
    address={Dordrecht},
    year=1994,
    pages={51--67},
    annote={Describes COBYLA algorithm for derivative free optimization of a
        function over a constrained domain.},
}
%rrrrrrrrrrrrr
@inproceedings{raina+battle+lee+packer+ng:2007,
    title={Self-taught learning: Transfer Learning from Unlabeled Data},
    author={R. Raina and A. Battle and H. Lee and B. Packer and A. Y. Ng},
    booktitle=icml07,
    year=2007,
    location = {Corvallis, OR},
    editor = {Z. Ghahramani},
    pages = {759--766},
    publisher = {Omnipress},
}
@inproceedings{ranzato+boureau+lecun:2008,
    author={M. Ranzato and Y. Boureau and Y. {LeCun}},
    title={Sparse Feature Learning for Deep Belief Networks},
    pages={1185--1192},
    year=2008,
    booktitle=NIPS20,
    editor=NIPS20ed,
    publisher=NIPS20publ,
    annote={Introduces the SESM model (aka SPD?)},
}
@InProceedings{ranzato+hinton:2010,
  author =       {M. Ranzato and G. E. Hinton},
  title =        {Modeling Pixel Means and Covariance using Factorized Third-order {B}oltzmann Machines},
  crossref={CVPR:2010},
  pages = {},
  annote= {Introduces mcRBM, with good results on CIFAR-10, by extending previous cRBM model (Ranzato, Krizhevsky, Hinton 2010).}
}
@InProceedings{ranzato+krizhevsky+hinton:2010,
  author =       {M. Ranzato and A. Krizhevsky and G. E. Hinton},
  title =        {Factored 3-Way Restricted {B}oltzmann Machines For Modeling Natural Images},
  crossref = {AISTATS:2010},
}
@inproceedings{ranzato+mnih+hinton:2010,
    author = {M. Ranzato and V. Mnih and G. Hinton},
    title = {Generating more Realistic Images using Gated {MRF}'s},
    pages = {2002--2010},
    crossref={NIPS23},
}
%marcaurelio:2006,
@InProceedings{ranzato+poultney+chopra+lecun:2007,
  author =       "M. Ranzato and C. Poultney and
                 S. Chopra and Y. {LeCun}",
  title =        "Efficient Learning of Sparse Representations with an Energy-Based Model",
  pages = {1137--1144},
  crossref = {NIPS19},
}
@article{rao+ballard:1999,
    author={R. P. N. Rao and D. H. Ballard},
    title={Predictive Coding in the Visual Cortex: a Functional Interpretation
        of Some Extra-Classical Receptive-Field Effects},
    journal={Nature Neuroscience},
    year={1999},
    volume=2,
    number=1,
    month=jan,
}
@book{rasmussen+williams:2006,
    author={C. E. Rasmussen and C. K. I. Williams},
    title={Gaussian Processes for Machine Learning},
    year=2006,
    publisher={{MIT} Press},
    path={reading/2006/RW.pdf},
    annote={Gaussian process textbook.}
}
@inproceedings{reid:1989,
    title = {Rapid Training of Higher-Order Neural Networks for Invariant Pattern
        Recognition},
    author = {Reid, M. B. and Spirkovska, L. and Ochoa, E.},
    crossref = {IJCNN:1989},
}
@article{rhodes:2008,
    author={P. A. Rhodes},
    title={Recoding Patterns of Sensory Input: Higher-Order Features and the Function of Nonlinear Dendritic Trees},
    journal={Neural Computation},
    volume={20},
    number=8,
    year={2008},
    pages={2000--2036},
}
@article{riesenhuber+poggio:1999,
    author={M. Riesenhuber and T. Poggio},
    title={Hierarchical Models of Object Recognition in Cortex},
    journal={Nature Neuroscience},
    year={1999},
    volume=2,
    pages={1019--1025},
    annote={Describes HMAX model.}
}
@book{rosenblatt:1962,
    author={F. Rosenblatt},
    title={Principles of Neurodynamics},
    publisher={Spartam Books},
    year=1962,
}
@article{rozell:2008,
    author={C. J. Rozell and D. H. Johnson and R. G. Baraniuk and B. A. Olshausen},
    title={Sparse Coding via Thresholding and Local Competition in Neural
        Circuits},
    year={2008},
    journal=nc,
    volume={20},
    pages={2526--2563},
    annote={
        Introduces a dynamical system to solve a variety of sparse-coding
            problems.  Different sparsity constraints lead to different
            non-linearities.  The work also considers neuron stability over time as an
            important objective.  Coding strategies are evaluated on videos.
            ***FINDINGS
            ***ALGORITHMS
    }
}
@Book{Rumelhart86a,
  author =       "D. E. Rumelhart and J. L. McClelland and the PDP
                 Research Group",
  title =        "Parallel Distributed Processing: Explorations in the
                 Microstructure of Cognition",
  volume =       "1",
  publisher =    "MIT Press",
  address =      "Cambridge",
  year =         "1986",
}
@Article{Rumelhart86b,
  author =       "D. E. Rumelhart and G. E. Hinton and R. J. Williams",
  title =        "Learning Representations by Back-Propagating Errors",
  journal =      "Nature",
  volume =       "323",
  pages =        "533--536",
  year =         "1986",
}
@InCollection{Rumelhart86c,
  author =       "D. E. Rumelhart and G. E. Hinton and R. J. Williams",
  editor =       "D. E. Rumelhart and J. L. McClelland",
  booktitle =    pdp,
  title =        "Learning Internal Representations by Error
                 Propagation",
  chapter =      "8",
  volume =       "1",
  publisher =    "MIT Press",
  address =      "Cambridge",
  pages =        "318--362",
  year =         "1986",
}
@article{rust+schwartz+movshon+simoncelli:2005,
    author      = {N. Rust and O. Schwartz and J. A. Movshon and E. Simoncelli},
    title       = {Spatiotemporal Elements of Macaque {V1} Receptive Fields},
    journal     = {Neuron},
    volume      = {46},
    number      = {6},
    pages       = {945-956},
    year        = {2005}
}
@article{rust+mante+simoncelli+movshon:2006,
    author = {N. C. Rust and V. Mante and E. P. Simoncelli and J. A. Movshon},
    year = {2006},
    title = {How MT Cells Analyze the Motion of Visual Patterns},
    journal = {Nature Neuroscience},
    volume = {9},
    number = {11},
    pages = {1421-1431},
}
%sss
@InProceedings{salakhutdinov+hinton:2009,
  author =       "R. Salakhutdinov and G. E. Hinton",
  title =        "Deep {B}oltzmann Machines",
  pages =        "448--455",
  crossref = {AISTATS:2009},
  annote={Describes technique of variational positive-phase inference and PCD negative-phase inference along with stacking to train 2-layers of RBM together. Gives likelihood and classification (0.95\%) on MNIST, classification on NORB (10.8\%)}
}
@inproceedings{salakhutdinov+larochelle:2010,
 author = {R. Salakhutdinov and H. Larochelle},
 title = {Efficient Learning of Deep {Boltzmann} Machines},
 pages = {693--700},
 crossref = {AISTATS:2010},
}
@InProceedings{salakhutdinov+mnih+hinton:2007,
  author =       "R. Salakhutdinov and A. Mnih and G. E. Hinton",
  booktitle =    ICML07,
  editor =       ICML07ed,
  publisher =    ICML07publ,
  title =        "Restricted {Boltzmann} Machines for Collaborative Filtering",
  address =      "New York, NY",
  pages =        "791--798",
  year =         "2007",
  location =     "Corvalis, Oregon",
}
@article{salinas:2001,
    author = {Salinas, E. and Abbott, L. F.},
    title = {Coordinate Transformations in the Visual System: How to Generate Gain Fields and What to Compute with Them},
    journal = {Progress in Brain Research},
    year = {2001},
    volume = {130},
    pages = {175-190},
}
@article{sato+kawamura+iwai:1980,
    author={T. Sato and T. Kawamura and E. Iwai},
    title={Responsiveness of Inferotemporal Single Units to Visual Pattern Stimuli in Monkeys Performing Discrimination},
    journal={Experimental Brain Research},
    volume=38,
    pages={313-319},
    year=1980,
}
@Article{scholkopf:1999,
    author = {B. Sch{\"o}lkopf and S. Mika and C. J. C. Burges and P. Knirsch 
        and K. M{\"u}ller and G. R{\"a}tsch and A. J. Smola},
    title = {Input Space Versus Feature Space in Kernel-Based Methods},
    journal = {IEEE Trans. Neural Networks},
    year = {1999},
    volume = {100},
    number = {5},
    pages = {1000--1017},
}
@article{schraudolph:2002,
    author={N. N. Schraudolph},
    title={Fast Curvature Matrix-Vector Products for Second-Order Gradient
        Descent},
    journal=nc,
    volume=14,
    number=7,
    pages={1723--1738},
    year=2002,
}
@inproceedings{schraudolph+graepel:2003,
    author={N. N. Schraudolph and T. Graepel},
    title={Combining Conjugate Direction Methods with Stochastic Approximation
        of Gradients},
    booktitle=aistats03,
    pages={7--13},
    publisher=aistats03publ,
    year=2003,
}
}
@inproceedings{schraudolph+yu+gunter:2007,
    author={N. N. Schraudolph and J. Yu and S. G{\"u}nter},
    title={A Stochastic Quasi-Newton Method for Online Convex Optimization},
    booktitle=aistats07,
    pages={436--443},
    publisher=jmlr,
    year=2007,
}
@article{schwartz+simoncelli:2001,
    author={O. Schwartz and E. P. Simoncelli},
    title={Natural Signal Statistics and Sensory Gain Control},
    journal={Nature Neuroscience},
    month=aug,
    year={2001},
    annote={},
}
@inproceedings{sermanet+kavukcuoglu+lecun:2009,
    title = "{EBLearn}: Open-Source Energy-Based Learning in {C++}",
    author = "Sermanet, P. and Kavukcuoglu, K. and {LeCun}, Y.",
    booktitle="Proc. International Conference on Tools with Artificial Intelligence (ICTAI'09)",
    publisher = "IEEE",
    year = "2009",
}
@Article{Serre2007,
  author =       "T. Serre and G. Kreiman and M. Kouh and C. Cadieu and
                 U. Knoblich and T. Poggio",
  title =        "A quantitative theory of immediate visual
                 recognition",
  journal =      "Progress in Brain Research, Computational
                 Neuroscience: Theoretical Insights into Brain
                 Function",
  volume =       "165",
  pages =        "33--56",
  year =         "2007",
}
@article{serre+oliva+poggio:2007,
    author={T. Serre and A. Oliva and T. Poggio},
    title={A Feedforward Architecture Accounts for Rapid Categorization},
    journal=pnas,
    doi={10.1073/pnas.0700622104},
    volume=104,
    number=15,
    pages={6424--6429},
    year={2007},
    annote={}
}
@article{shams+malsburg:2002,
    author={L. Shams and C. {von der Malsburg}},
    title={The Role of Complex Cells in Object Recognition},
    journal={Vision Research},
    volume=42,
    number=22,
    pages={2547--2554},
    year=2002,
    annote={Uses a filter bank of Gabor energy models of V1 complex cells to
        represent images.  For several realistic images, the work shows that
            the internal representation can be decoded using a gradient-based
            inversion algorithm, and the recovered image is readily
            recognizeable.
            The representation is robust to changes in polarity, which the
            authors argue is important because changes in lighting and
            background often induce polarity reversals.
    }
}
@article{shannon:1948,
    author={C. E. Shannon},
    title={A Mathematical Theory of Communication},
    journal={Bell System Technical Journal},
    volume=27,
    pages={379--423 and 623–656},
    month={July and October},
    year={1948},
}
@inproceedings{shin:1991,
    title = {The Pi-Sigma Network: An Efficient Higher-Order Neural Network for
        Pattern Classification and Function Approximation},
    author = {Y. Shin and J. Ghosh},
    crossref = {IJCNN:1991},
}
@inproceedings{simard:2003,
    title = {Best Practice for Convolutional Neural Networks Applied to Visual Document Analysis},
    author = {P. Y. Simard and D. Steinkraus and J. Platt},
    year = {2003},
    address = {Los Alamitos},
    pages = {958-962},
    booktitle = {International Conference on Document Analysis and Recogntion
        (ICDAR), IEEE Computer Society}
}
@article{simoncelli+olshausen:1999,
    title={Natural Image Statistics and Neural Representations},
    author={E. P. Simoncelli and B. A. Olshausen},
    journal={Annual Reviews in Neuroscience},
    year=2001,
    volume=24,
    pages={1193--1216},
    annote={Review of ICA, sparse coding, dependence of codes, includes discussion at end of fast adaptation by nervous system.}
}
@incollection{smolensky:1986,
    author={P. Smolensky},
    title={Information Processing in Dynamical Systems: Foundations of Harmony
        Theory},
    booktitle=pdp1,
    publisher={MIT Press},
    address={Cambridge, MA},
    editor=pdp1ed,
    year=1986,
}
@inproceedings{spirkovska:1990,
    author={Spirkovska, L. and Reid, M. B.},
    title={Connectivity Strategies for Higher-Order Neural Networks Applied to Pattern Recognition},
    booktitle={Proc. of International Joint Conference on Neural Networks ({IJCNN})},
    year={1990},
    volume=1,
    pages={21-26},
    doi={10.1109/IJCNN.1990.137538},
}
@article{sprekeler+michaelis+wiskott:2007,
    author={Sprekeler, H. and Michaelis, C. and Wiskott, L.},
    title={Slowness: An Objective for Spike-Timing-Dependent Plasticity?},
    month=jun,
    year=2007,
    journal={PLoS Computational Biology},
    volume=3,
    number=6,
    pages={e112},
    doi={doi:10.1371/journal.pcbi.0030112},
}
@article{srinivasan+ramakrishnan:2011,
    title={Parameter Screening and Optimisation for {ILP} using Designed
        Experiments},
    author={A. Srinivasan and G. Ramakrishnan},
    journal=JMLR,
    volume=12,
    month=feb,
    pages={627--662},
    year={2011},
    annote={Advocates response surface modeling for ILP hyper-parameter optimization.
    They use linear models, show some results on problems that I don't recognize.}
}
@article{stevenson+cronin+sur+kording:2010,
    author={Stevenson, I. H. and Cronin, B. and Sur, M. and K{\"o}rding, K. P.},
    title={Sensory Adaptation and Short Term Plasticity as {Bayesian} Correction for a Changing Brain},
    journal={PLoS ONE},
    year=2010,
    volume=5,
    number=8,
    pages={e12436},
    doi={10.1371/journal.pone.0012436},
}
@incollection{sutskever+hinton+taylor:2009,
 title = {The Recurrent Temporal Restricted {Boltzmann} Machine},
 author = {I. Sutskever and G. E. Hinton and G. Taylor},
 pages = {1601--1608},
 crossref={NIPS21},
}
@book{sutton+barto:1998,
    author={R. S. Sutton and A. G. Barto},
    title={Reinforcement Learning: An Introduction},
    publisher={MIT Press},
    address={Cambridge, MA},
    year={1998},
}
@article{swindale:2008,
    author={N. V. Swindale},
    title={Visual Map},
    year=2008,
    journal={Scholarpedia},
    volume=3,
    number=6,
    pages=4607,
    doi={10.4249/scholarpedia.4607},
    annote={}
}
%ttt
@InProceedings{taylor+hinton:2009,
  author =    {G. Taylor and G. E. Hinton},
  title =     {Factored Conditional Restricted {Boltzmann} Machines for Modeling Motion Style},
  pages =     {1025--1032},
  crossref = {ICML:2009},
}
@Article{tenenbaum+freeman:2000,
  author =       "J. B. Tenenbaum and W. T. Freeman",
  title =        "Separating Style and Content with Bilinear Models",
  journal =      "Neural Computation",
  volume =       "12",
  number =       "6",
  pages =        "1247--1283",
  year =         "2000",
}
@inproceedings{tieleman:2008,
    author={T. Tieleman},
    title={Training restricted {Boltzmann} Machines using Approximations to the Likelihood Gradient},
    booktitle=icml08,
    editor=icml08ed,
    publisher=icml08publ,
    year=2008,
    pages={1064-1071},
}
@article{tootell+silverman+switkes+devalois:1982,
    author={R. B. Tootell and M. S. Silverman and E. Switkes and R. L. {De Valois}},
    title={Deoxyglucose Analysis of Retinotopic Organization in Primate Striate Cortex},
    journal={Science},
    volume=218,
    number=4575,
    pages={902--904},
    year=1982,
    doi={10.1126/science.7134981},
    annote={Has a really cool image of the V1 retinotopic map, which was reprinted in the Dayan and Abbott book.}
}
@article{torralba+fergus+freeman:2008,
    author={A. Torralba and R. Fergus and W. T. Freeman},
    title={80 Million Tiny Images: A Large Dataset For Non-Parametric Object and Scene Recognition},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
    volume=30,
    number=11,
    pages={1958--1970},
    year=2008,
    annote = {Introduces the Tiny Images dataset - see also CIFAR-10 (Krizhevsky,2009).},
}
@article{tresp:2000,
    title={A Bayesian Committee Machine},
    author={V. Tresp},
    pages={2719--2741},
    journal={Neural Computation},
    volume={12},
    year=2000,
    annote={Iain Murray said that this works (says George Dahl).  Later, Iain explained
        the principle at the whiteboard: ask members of an ensemble to jointly
            produce estimates for several test points.  The posterior over
            functions is the *product* of the posteriors from the members.  This
            works out very conveniently for Gaussian Process Regression.
    }
}
@inproceedings{triefenbach+jalalvand+schrauwen+martens:2010,
    title={Phoneme Recognition with Large Hierarchical Reservoirs},
    author={F. Triefenbach and A. Jalalvand and B. Schrauwen and J. Martens},
    booktitle={{NIPS}},
    year={2010},
    annote={
        Solid application paper explains how Multi-layer Reservoir approach is
        used to good results on TIMIT.
        There are nice figures to illustrate the advantage of depth, for
        reservoirs of various sizes.
        Their own results are not as good as the other scores that they include
        in Table 2 though!  Also, the model appears very slow, requiring 2 days
        to train and 2 seconds to decode just 1 second of speech on a 3GHz
        machine.
    },
}
@INPROCEEDINGS{turian+bergstra+bengio:2009,
     author = {Turian, J. and Bergstra, J. and Bengio, Y.},
      month = jun,
      title = {Quadratic Features and Deep Architectures for Chunking},
  booktitle = {North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT)},
       year = {2009},
      pages = {245--248},
  publisher = {Association for Computational Linguistics},
    address = {Boulder, Colorado},
   abstract = {We experiment with several chunking models. Deeper architectures achieve better generalization. Quadratic filters, a simplification of theoretical model of V1 complex cells, reliably increase accuracy. In fact, logistic regression with quadratic filters outperforms a standard single hidden layer neural network. Adding quadratic filters to logistic regression is almost as effective as feature engineering. Despite predicting each output label independently, our model is competitive with ones that use previous decisions.}
}
@article{turing:1950,
    author={Turing, A.},
    year=1950,
    title={Computing Machinery and Intelligence},
    journal={Mind},
    volume=59,
    number=236,
    pages={433--60},
    annote = {Introduces the Turing Test.}
}
@article{turnbull+elkan:2005,
    author={D. Turnbull and C. Elkan},
    year={2005},
    title={Fast Recognition of Musical Genres Using RBF Networks},
    journal = {{IEEE} Transactions on Knowledge and Data Engineering},
    volume=17,
    number=4,
    month={April},
    annote = {Used clustering to initialize the means and variances of an RBF
        network.  Scored 72\% on the Tzanetakis Dataset.  Used "Forward Stepwise
            Feature Selection" to select features, and had good results with
            10-15 features.
    }
}
@misc{turner+sahani:2007,
    author={R. Turner and M. Sahani},
    title={A Maximum-Likelihood Interpretation for Slow Feature Analysis},
    year={2007},
    journal={Neural Computation},
    volume=19,
    number=4,
    pages={1022-1038},
    annote={
        Derivation similar to probabilistic PCA for SFA.
        They show that learning with a slow feature prior
        results in a model similar to a Kalman filter or hidden Markov model.
        Extensions are proposed,
        including one about bilinear modeling, but not explored
        empirically.
    }
}
%uuu
@incollection{ueberhuber:1997,
    author={C. W. Ueberhuber},
    booktitle={Numerical Computation (Volume 2)},
    title={Minimization Methods},
    pages={325--335},
    year={1997},
    publisher={Springer},
    annote= {Description of conjugate gradient and BFGS minimization methods implemented in GNU Scientific Library.},
}
@incollection{ungerleider+mishkin:1982,
    author={Ungerleider and Mishkin},
    year={1982},
    title={Two Cortical Visual Systems},
    editor={D. J. Ingle and M. A. Goodale and R. J. W. Mansfield},
    booktitle={Analysis of Visual Behavior},
    publisher={MIT Press},
}
%vvv
@book{vapnik:1982,
    author={V. N. Vapnik},
    title={Estimation of Dependences based on Empirical Data: Addendum 1},
    publisher={Springer Verlag},
    year=1982,
}
@book{vapnik:1989,
    author={V. N. Vapnik},
    title={Statistical Learning Theory},
    year=1989,
    publisher={Wiley-Interscience},
    annote={Describes VC theory}
}
@Book{vapnik:1995,
  author =       "V. N. Vapnik",
  title =        "The Nature of Statistical Learning Theory",
  publisher =    "Springer",
  address =      "New York",
  year =         "1995",
}
@article{vapnik+chervonenkis:1971,
    author={V. N. Vapnik and A. Chervonenkis},
    title={On the Uniform Convergence of the Relative Frequencies of Events to
        their Probablities},
    journal={Theory Prob. Appl.},
    volume=16,
    year=1971,
    pages={264-280},
}
@article{vapnik+levin+lecun:1994,
    author={V. N. Vapnik and E. Levin and Y. {LeCun}},
    title={Measuring the {VC}-Dimension of a Learning Machine},
    journal={Neural Computation},
    volume=6,
    number=5,
    pages={851 - 876},
    year=1994,
    doi={10.1162/neco.1994.6.5.851}
}
@inproceedings{vincent+larochelle+bengio+manzagol:2008,
  author =       "P. Vincent and H. Larochelle and Y. Bengio
                 and P. Manzagol",
  booktitle =    ICML08,
  editor =       ICML08ed,
  publisher =    ICML08publ,
  title =        "Extracting and Composing Robust Features with
                 Denoising Autoencoders",
  year =         "2008",
  pages =        "1096--1103"
}
@article{vincent+larochelle+lajoie+bengio+manzagol:2010,
    author={P. Vincent and H. Larochelle and I. Lajoie and Y. Bengio and P. A. Manzagol},
    title={Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},
    journal={Machine Learning Research},
    volume={11},
    pages={3371--3408},
    year={2010}
}
@article{vonmelchner+pallas+sur:2000,
    author={{Von Melchner}, L. and Pallas, S. L. and Sur, M.},
    title={Visual Behavior Mediated by Retinal Projections Directed to the Auditory Pathway},
    journal={Nature},
    volume=404,
    pages={871--876},
    year={2000},
    annote = {Describes the experiment in which the optic and auditory nerves of baby ferrets are switched,
        but the animals still develop with senses of sight and hearing (though with poorer visual acuity).}
}
@article{vorontsov:2004,
    author={K. V. Vorontsov},
    title={Combinatorial Substantiation of Learning Algorithms},
    journal={Computational Mathematics and Mathematical Physics},
    volume=44,
    number=11,
    pages={1997--2009},
    year=2004,
    annote = {Transductive approach to data- and algorithm-dependent generalization bounds.}
}
@article{vural:2009,
    author={V. Vural and G. Fung and B. Krishnapuram and J. G. Dy and B. Rao},
    title={Using Local Dependencies within Batches to Improve Large Margin
        Classifiers},
    journal = jmlr,
    year=2009,
    volume=10,
    month=February,
    pages={183--206},
}
%wwwwwwwwww
@incollection{wainright+schwartz+simoncelli:2002,
    title={Natural Image Statistics and Divisive Normalization:
        Modeling Nonlinearity and Adaptation in Cortical Neurons},
    author={M. J. Wainwright and O. Schwartz and E. P. Simoncelli},
    booktitle={Probabilistic Models of the Brain: Perception and Neural Function},
    pages={203--222},
    publisher={MIT Press},
    year={2002},
}
@inproceedings{wainright+simoncelli:2000,
AUTHOR= "M. J. Wainwright and E. P. Simoncelli",
TITLE= "Scale Mixtures of {Gaussians} and the Statistics of Natural Images",
PAGES= "855--861",
crossref={NIPS12},
annote={Introduces Gaussian Scale Mixture Model},
}
@article{wang+shen+pan:2009,
    title={On Efficient Large Margin Semisupervised Learning: Method and
        Theory},
    author={J. Wang and X. Shen and W. Pan},
    journal = jmlr,
    volume = {10},
    month="March",
    year={2009},
    pages={719--742},
}
@book{weise:2009,
    author = {T. Weise},
    title = {Global Optimization Algorithms - Theory and Application},
    year = {2009},
    howpublished = {Online as e-book},
    edition = {Second},
    institution = {University of Kassel, Distributed Systems Group},
    organization = {University of Kassel, Distributed Systems Group},
    publisher = {Self-Published},
    copyright = {Copyright (c) 2006-2009 Thomas Weise, licensed under GNU FDL},
    note={Online available at http://www.it-weise.de/},
    annote={Survey introduction to global optimization, especially evolutionary
        methods (heuristic and metaheuristic methods).}
}
@inproceedings{weiss:2007,
    title = {Learning Compressed Sensing},
    author = {Y. Weiss and H. S. Chang and W. T. Freeman},
    year = {2007},
    booktitle = {Allerton},
    annote = {
        Introduces {\em uncertain components analysis} (UCA) as an alternative to PCA,
        ICA, and random projections.  This technique (based on the InfoMax
        principle) produces a more efficient measurement matrix for images.
    },
}
@article{welling:2002,
    author={M. Welling and S. Osindero and G. E. Hinton},
    year = {2002},
    title = {Learning Sparse Topographic Representations with Products of Student-t Distributions},
    booktitle = {NIPS},
    annote = {Introduces POT algorithm (product of student-t), for fitting Energy models with quadratic
        inputs.  This algorithm learns beautiful filters; arranged
        topographically they form pinwheels.
    }
}
@inproceedings{welling+hinton+osindero:2003,
  author =       {Welling, M. and Hinton, G. E. and Osindero, S.},
  title =        {Learning Sparse Topographic Representations with Products of {S}tudent-t Distributions},
  pages     = "1359--1366",
  crossref={NIPS15},
}
@InProceedings{weston+ratle+collobert:2008,
  author =       "J. Weston and F. Ratle and R. Collobert",
  booktitle =    ICML08,
  editor =       ICML08ed,
  publisher =    ICML08publ,
  title =        "Deep Learning via Semi-Supervised Embedding",
  year =         "2008",
  pages =        {1168--1175},
  address =      {Helsinki, Finland},
  doi =          {10.1145/1390156.1390303},
}
@Unpublished{williams:1993,
  author =       {C. K. I. Williams},
  title =        {Continuous-valued {B}oltzmann Machines},
  note =         {Unpublished Manuscript},
  month =     mar,
  year =      {1993}
}
@InProceedings{williams:2001,
  author =       "C. K. I. Williams",
  title =        "On a Connection between Kernel {PCA} and Metric Multidimensional Scaling",
  pages =        "675--681",
  crossref={NIPS13},
}
@article{wilson+cowan:1972,
    author = {H. R. Wilson and J. D. Cowan},
    title = {Excitatory and Inhibitory Interactions in Localized Populations of Model Neurons},
    journal = {Biophysical Journal},
    pages = {1--24},
    volume = {12},
    number = {1},
    month = {Jan},
    year = {1972},
}
@inproceedings{wingate:2006,
    author={D. Wingate and S. Singh},
    title={Kernel Predictive Linear {Gaussian} Models for Nonlinear Stochastic
        Dynamical Systems},
    booktitle=icml06,
    editor=icml06ed,
    publisher=icml06publ,
    year={2006},
    pages={1017--1024},
}
@inproceedings{wingate:2007relational,
    author={D. Wingate and V. Soni and B. Wolfe and S. Singh},
    title={Relational Knowledge with Predictive State Representations},
    booktitle={International Joint Conference on Artificial Intelligence
        {(IJCAI)}},
    year={2007},
    pages={2035--2040},
}
@inproceedings{wingate:2007exponential,
    author={D. Wingate and S. Singh},
    title={Exponential Family Predictive Representations of State},
    booktitle={Neural Information Processing Systems {(NIPS)}},
    year={2007},
}
@phdthesis{wingate:2008thesis,
    author={D. Wingate},
    title={Exponential Family Predictive Representations of State},
    year={2008},
    school={University of Alberta},
}
@article{wiskott:2002,
    author =       "L. Wiskott and T. Sejnowski",
    year =         "2002",
    title = {Slow Feature Analysis: Unsupervised Learning of Invariances},
    journal =      "Neural Computation",
    volume =       "14",
    number =       "4",
    pages =        "715--770",
    abstract =     "Invariant features of temporally varying signals are useful for
        analysis and classification. Slow feature analysis (SFA) is a new method for
        learning invariant or slowly varying features from a vectorial input signal.
        SFA is based on a non-linear expansion of the input signal and application
        of principal component analysis to this expanded signal and its time
        derivative.  It is guaranteed to find the optimal solution within a family
        of functions directly and can learn to extract a large number of
        decorrelated features, which are ordered by their degree of invariance. SFA
        can be applied hierarchically to process high dimensional input signals and
        to extract complex features. Slow feature analysis is applied first to
        complex cell tuning properties based on simple cell output including
        disparity and motion.  Then, more complicated input-output functions are
        learned by repeated application of SFA. Finally, a hierarchical network of
        SFA-modules is presented as a simple model of the visual system.  The same
        unstructured network can learn translation, size, rotation, contrast, or, to
        a lesser degree, illumination invariance for one-dimensional objects,
        depending only on the training stimulus. Surprisingly, only a few training
        objects sufficed to achieve good generalization to new objects.  The
        generated representation is suitable for object recognition.  Performance
        degrades, if the network is trained to learn multiple invariances
        simultaneously.",
    annote={{
        Algorithm:
        \begin{enumerate}
            \item Normalize the data (mean 0, variance 1)
            \item Project normalized data into large feature space (they used 1st and 2nd order monomials)
            \item Whiten/sphere the feature space
            \item Identify eigenvectors in the covariance of feature derivatives
            \item Output uses the eigenvectors to produce the SFA embedding of the features.
        \end{enumerate}
        The algorithm is stackable: it might make sense to repeat it on the embedding.

        The algorithm identifies the slowest-moving components in the feature
        space.

        This algorithm does not include or require any backpropagation from
        higher layers to lower ones.
    }}
}
@article{wyss+konig+verschure:2006,
    author={Wyss, R. and K{\"o}nig, P. and Verschure, P.},
    year={2006},
    title={A model of the Ventral Visual System Based on Temporal Stability and
        Local Memory},
    journal={{PLoS} Biology},
    volume=4,
    number=120,
}
%xxxxx
%yyyyy
@InProceedings{younes:1998,
    author = {L. Younes},
    title = {On The Convergence Of {Markovian} Stochastic Algorithms With Rapidly Decreasing Ergodicity Rates},
    booktitle = {Stochastics and Stochastics Models},
    year = {1998},
    pages = {177--228},
    annote = {Introduces Persistent Contrastive Divergence (PCD) earlier than Tieleman, 2008.}
}
%zzzzzz
@inproceedings{zemel+williams+mozer:1993,
 author = {Zemel, R. S. and Williams, C. K. I. and Mozer, M.},
 title = {Directional-Unit {Boltzmann} Machines},
 pages = {172--179},
 crossref = {NIPS5},
 annote = {}
}
@article{zetzsche+krieger+wegmann:1999,
    title={The Atoms of Vision: {Cartesian} or Polar?},
    author={C. Zetzsche and G. Krieger and B. Wegmann},
    journal={J. of the Optical Society of America A},
    volume=16,
    number=7,
    pages={1554--1565},
    year={1999},
    doi={10.1364/JOSAA.16.001554},
    annote={}
}
@article{zilles+etal:2011,
    title={Models of Cooperative Teaching and Learning},
    author={S. Zilles, S. Lange, R. Holte, M. Zinkevich},
    journal=JMLR,
    volume=12,
    month=feb,
    pages={349--384},
    year={2011},
    annote={}
}
@inproceedings{zinkevich+etal:2010,
    author={M. A. Zinkevich and M. Weimer and A. Smola and L. Li},
    title={Parallelized Stochastic Gradient Descent},
    crossref={NIPS23},
    annote={
        Proof that a simple parallel implementation of SGD is just as efficient
        as a long-running serial implementation of SGD for optimizing a
        convex loss function in wx+b.
        The proof (and algorithm too it seems) relies heavily on the convexity of
        the problem, so that solutions obtained on different computers (even
        intermediate ones) can be averaged together.
        This sort of algorithm does present itself for the training of
        non-convex functions, but it would seem that the population of current
        estimates must converge to a locally-convex region of the space before
        the algorithm would {\it work} in any way.  But at that point, it would
        still permit training a single model on huge (linear in number of computers) amounts of data.
    }
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END OF ALPHABETIZED LISTINGS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proceedings (alphabetized)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@proceedings{AISTATS:2009,
  year =         "2009",
  booktitle =    aistats09,
  volume =       5,
  address =     "Clearwater, FL",
  month = apr,
}
@proceedings{AISTATS:2010,
    title=aistats10title,
    booktitle=aistats10title,
    year=2010,
    editor={Y. W. Teh and M. Titterington},
    publisher=aistats10publ,
    address="",
    location = {Sardinia, Italy},
}
@proceedings{AISTATS:2011,
    title=aistats11title,
    booktitle=aistats11title,
    year=2011,
    publisher=aistats11publ,
    address="",
    location = {Fort Lauderdale, FL},
}
@proceedings{CVPR:2008,
    title=cvpr08,
    booktitle=cvpr08,
    year=2008,
    publisher={IEEE Computer Society},
}
@proceedings{CVPR:2009,
  title=cvpr09,
  booktitle=cvpr09,
  year={2009},
  publisher = "IEEE Press",
}
@proceedings{CVPR:2010,
  title = cvpr10,
  booktitle = cvpr10,
  publisher = "IEEE Press",
  year = 2010,
}
@proceedings{ICML:2007,
      year = {2007},
  location = {Corvallis, OR},
  title = ICML07,
  booktitle = ICML07,
  editor   = ICML07ed,
  publisher= ICML07publ,
}
@proceedings{ICML:2008,
    title = icml08,
    booktitle = icml08,
    editor=icml08ed,
    publisher=icml08publ,
    year = {2008},
}
@proceedings{ICML:2009,
  year =      2009,
  booktitle = ICML09,
  editor =    ICML09ed,
  address =   ICML09loc,
  month =     ICML09month,
  publisher = ICML09publ,
}
@proceedings{ICML:2010,
    booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
    year =      2010,
    editor =    {J. F{\"u}rnkranz and T. Joachims},
    address =   {Haifa, Israel},
    month =     {June},
    publisher = {Omnipress}
}
@proceedings{ICML:2011,
    year =      2010,
    booktitle = ICML11,
    editor =    ICML11ed,
    address =   ICML11addr,
    month =     ICML11month,
    publisher = ICML11publ,
}
@PROCEEDINGS{ICPC:2003,
    booktitle   = {Proceedings of {IEEE} International Conference on Image Processing},
    year 	= {2003},
    month	= {Sept},
    address = {Barcelona, Spain}
}
@proceedings{IJCNN:1991,
    title = {International Joint Conference on Neural Networks ({IJCNN})},
    booktitle = {International Joint Conference on Neural Networks ({IJCNN})},
    year = {1991},
    address = {Seattle, Washington, USA},
}
@proceedings{IJCNN:1989,
    title   = {International Joint Conference on Neural Networks ({IJCNN})},
    booktitle   = {International Joint Conference on Neural Networks ({IJCNN})},
    month   = {June},
    year    = {1989},
    address = {Washington, DC, USA},
}
@proceedings{IWCMC:2010,
    booktitle=IWCMC2010title,
    month=jun,
    publisher=IWCMC2010publ,
    address=IWCMC2010addr,
    year=2010,
}
@proceedings{NIPS5,
 booktitle = NIPS5ttl,
 year = {1993},
 editor = {},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA},
}
@proceedings{NIPS13,
  booktitle =    NIPS13ttl,
  year =         "2001",
  editor =       NIPS13ed,
  publisher =    "{MIT} Press",
}
@proceedings{NIPS15,
  title = NIPS15ttl,
  booktitle = NIPS15ttl,
  editor    = "S. Becker and S. Thrun and K. Obermayer",
  publisher = "MIT Press",
  address   = "Cambridge, MA",
  year      = "2003",
}
@proceedings{NIPS18,
    title=nips18,
    booktitle=nips18,
    editor = {Y. Weiss and B. Sch{\"o}lkopf and J. Platt},
    publisher = {MIT Press},
    address = {Cambridge, MA},
    year=2006,
}
@PROCEEDINGS{NIPS19,
   title =    NIPS19,
   booktitle =    NIPS19,
   year 	= {2007},
   publisher =    "MIT Press",
   editor =       NIPS19ed,
}
@proceedings{NIPS21,
 title = NIPS21,
 booktitle = NIPS21,
 editor = NIPS21ed,
 publisher = NIPS21publ,
 year = {2009},
}
@proceedings{NIPS22,
 title = NIPS22,
 booktitle = NIPS22,
 editor = NIPS22ed,
 publisher = NIPS22publ,
 year = {2009},
}
@proceedings{NIPS23,
 title = NIPS23,
 booktitle = NIPS23,
 editor = NIPS23ed,
 publisher = NIPS23publ,
 year = {2010},
}
@proceedings{spie:2007,
    title="Proc. SPIE, Conf. on Human Vision and Electronic Imaging {XII}",
    booktitle= "Proc. SPIE, Conf. on Human Vision and Electronic Imaging {XII}",
    year=2007,
    EDITOR= "B. E. Rogowitz and T. N. Pappas and S. J. Daly",
    PUBLISHER= "Society of Photo-Optical Instrumentation",
    VOLUME= 6492,
}
@proceedings{uai:2007,
 year = 2007,
 booktitle = UAI07,
}
